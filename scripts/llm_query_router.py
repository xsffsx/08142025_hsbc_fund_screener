#!/usr/bin/env python3
"""
åŸºäº LLM çš„æ™ºèƒ½æŸ¥è¯¢è·¯ç”±å™¨
é€šè¿‡ LLM åˆ†æè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œæ™ºèƒ½è·¯ç”±åˆ°æœ€ä¼˜çš„æŸ¥è¯¢ç­–ç•¥
"""

import json
import time
from typing import Dict, Any, List, Optional
from enum import Enum
from dataclasses import dataclass

try:
    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import JsonOutputParser
    from pydantic import BaseModel, Field
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
    print("è¯·å®‰è£…: pip install langchain langchain-openai")
    exit(1)

class QueryType(str, Enum):
    """æŸ¥è¯¢ç±»å‹æšä¸¾"""
    SIMPLE = "simple"           # ç®€å•æ ‡é‡æŸ¥è¯¢
    COMPLEX = "complex"         # å¤æ‚JSONBæŸ¥è¯¢  
    HYBRID = "hybrid"           # æ··åˆæŸ¥è¯¢
    AGGREGATION = "aggregation" # èšåˆç»Ÿè®¡æŸ¥è¯¢
    ANALYTICAL = "analytical"   # åˆ†æå‹æŸ¥è¯¢

class QueryComplexity(str, Enum):
    """æŸ¥è¯¢å¤æ‚åº¦"""
    LOW = "low"       # ä½å¤æ‚åº¦ï¼šå•è¡¨ç®€å•ç­›é€‰
    MEDIUM = "medium" # ä¸­å¤æ‚åº¦ï¼šJSONBè·¯å¾„æŸ¥è¯¢
    HIGH = "high"     # é«˜å¤æ‚åº¦ï¼šå¤šå±‚JSONBå±•å¼€+èšåˆ

@dataclass
class QueryAnalysis(BaseModel):
    """æŸ¥è¯¢åˆ†æç»“æœ"""
    query_type: QueryType = Field(description="æŸ¥è¯¢ç±»å‹")
    complexity: QueryComplexity = Field(description="æŸ¥è¯¢å¤æ‚åº¦")
    required_fields: List[str] = Field(description="éœ€è¦çš„å­—æ®µåˆ—è¡¨")
    jsonb_fields: List[str] = Field(description="æ¶‰åŠçš„JSONBå­—æ®µ")
    filters: List[str] = Field(description="ç­›é€‰æ¡ä»¶")
    aggregations: List[str] = Field(description="èšåˆæ“ä½œ")
    confidence: float = Field(description="åˆ†æç½®ä¿¡åº¦ 0-1")
    reasoning: str = Field(description="è·¯ç”±å†³ç­–ç†ç”±")
    suggested_strategy: str = Field(description="å»ºè®®çš„æŸ¥è¯¢ç­–ç•¥")

class LLMQueryRouter:
    """åŸºäº LLM çš„æ™ºèƒ½æŸ¥è¯¢è·¯ç”±å™¨"""
    
    def __init__(self, model: str = "gpt-3.5-turbo", temperature: float = 0.1):
        self.llm = ChatOpenAI(model=model, temperature=temperature)
        self.parser = JsonOutputParser(pydantic_object=QueryAnalysis)
        self.prompt = self._create_routing_prompt()
        self.chain = self.prompt | self.llm | self.parser
        
    def _create_routing_prompt(self) -> ChatPromptTemplate:
        """åˆ›å»ºæŸ¥è¯¢è·¯ç”±æç¤ºè¯"""
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åº“æŸ¥è¯¢åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æ HSBC åŸºé‡‘æ•°æ®æŸ¥è¯¢éœ€æ±‚ã€‚

æ•°æ®åº“ç»“æ„ï¼š
hsbc_fund_unified è¡¨åŒ…å« 1,407 ä¸ªåŸºé‡‘äº§å“ï¼Œé‡‡ç”¨æ··åˆå­˜å‚¨æ¶æ„ï¼š

ã€æ ‡é‡åˆ—ã€‘- ç”¨äºé«˜é¢‘ç®€å•æŸ¥è¯¢ï¼Œæœ‰ B-Tree ç´¢å¼•ï¼š
- product_code (ä¸»é”®), name, family_name, family_code
- risk_level (1-5), currency, hsbc_category_name  
- nav, nav_date, allow_buy, allow_sell, allow_sw_in
- assets_under_mgmt, expense_ratio, inception_date

ã€JSONBåˆ—ã€‘- ç”¨äºå¤æ‚ç»“æ„æŸ¥è¯¢ï¼Œæœ‰ GIN ç´¢å¼•ï¼š
- risk_json: 1/3/5/10å¹´é£é™©æŒ‡æ ‡æ•°ç»„
  ç»“æ„: [{"yearRisk": {"year": 1, "beta": 1.02, "alpha": 3.2, "stdDev": 24.91, "sharpeRatio": 1.40, "totalReturn": 43.56}}]
  
- top10_holdings: å‰åå¤§æŒä»“æ˜ç»†
  ç»“æ„: {"items": [{"securityName": "å…¬å¸å", "market": "US", "weighting": 8.45}], "asOfDate": "2025-07-31"}
  
- holding_allocation: èµ„äº§é…ç½®å’Œåœ°åŒºåˆ†å¸ƒ
  ç»“æ„: [{"methods": "assetAllocations", "breakdowns": [{"name": "Equity", "weighting": 95.19}]}]
  
- summary_cumulative: ç´¯è®¡æ”¶ç›Šç‡
  ç»“æ„: {"items": [{"period": "1Y", "totalReturn": 43.56}]}
  
- chart_timeseries: å†å²ä»·æ ¼æ—¶é—´åºåˆ—
  ç»“æ„: [{"date": "2022-08-16", "navPrice": 45.23}]

æŸ¥è¯¢ç±»å‹å®šä¹‰ï¼š
1. SIMPLE: ä»…ä½¿ç”¨æ ‡é‡åˆ—çš„ç®€å•ç­›é€‰å’Œæ’åº
2. COMPLEX: ä¸»è¦ä½¿ç”¨JSONBå­—æ®µçš„å¤æ‚æŸ¥è¯¢
3. HYBRID: æ ‡é‡åˆ—ç­›é€‰ + JSONBå­—æ®µå±•ç¤º
4. AGGREGATION: åŒ…å«COUNTã€SUMã€AVGç­‰èšåˆæ“ä½œ
5. ANALYTICAL: å¤æ‚åˆ†æï¼Œå¦‚è¶‹åŠ¿åˆ†æã€ç›¸å…³æ€§åˆ†æ

å¤æ‚åº¦è¯„ä¼°ï¼š
- LOW: å•è¡¨æ ‡é‡åˆ—æŸ¥è¯¢ï¼Œç®€å•WHEREæ¡ä»¶
- MEDIUM: æ¶‰åŠ1-2ä¸ªJSONBå­—æ®µçš„è·¯å¾„æŸ¥è¯¢
- HIGH: å¤šä¸ªJSONBå­—æ®µå±•å¼€ã€å¤æ‚èšåˆã€å¤šå±‚åµŒå¥—

è¯·åˆ†æç”¨æˆ·æŸ¥è¯¢å¹¶è¿”å›JSONæ ¼å¼çš„è·¯ç”±å†³ç­–ã€‚"""

        human_prompt = """ç”¨æˆ·æŸ¥è¯¢: {query}

è¯·åˆ†æè¿™ä¸ªæŸ¥è¯¢å¹¶è¿”å›è¯¦ç»†çš„è·¯ç”±å†³ç­–ï¼ŒåŒ…æ‹¬ï¼š
1. æŸ¥è¯¢ç±»å‹å’Œå¤æ‚åº¦
2. éœ€è¦çš„å­—æ®µå’ŒJSONBè·¯å¾„
3. ç­›é€‰æ¡ä»¶å’Œèšåˆæ“ä½œ
4. ç½®ä¿¡åº¦å’Œå†³ç­–ç†ç”±
5. å»ºè®®çš„æŸ¥è¯¢ç­–ç•¥

{format_instructions}"""

        return ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", human_prompt)
        ])
    
    def analyze_query(self, query: str) -> QueryAnalysis:
        """åˆ†ææŸ¥è¯¢å¹¶è¿”å›è·¯ç”±å†³ç­–"""
        try:
            result = self.chain.invoke({
                "query": query,
                "format_instructions": self.parser.get_format_instructions()
            })
            return QueryAnalysis(**result)
        except Exception as e:
            # é™çº§åˆ°ç®€å•åˆ†æ
            return self._fallback_analysis(query, str(e))
    
    def _fallback_analysis(self, query: str, error: str) -> QueryAnalysis:
        """LLMå¤±è´¥æ—¶çš„é™çº§åˆ†æ"""
        query_lower = query.lower()
        
        # ç®€å•å…³é”®è¯åŒ¹é…
        jsonb_keywords = ["æ”¶ç›Šç‡", "æŒä»“", "é£é™©æŒ‡æ ‡", "å†å²", "é…ç½®", "å¤æ™®", "beta", "alpha"]
        simple_keywords = ["é£é™©ç­‰çº§", "å¸ç§", "åŸºé‡‘å…¬å¸", "nav", "åç§°"]
        
        has_jsonb = any(keyword in query_lower for keyword in jsonb_keywords)
        has_simple = any(keyword in query_lower for keyword in simple_keywords)
        
        if has_jsonb and has_simple:
            query_type = QueryType.HYBRID
            complexity = QueryComplexity.MEDIUM
        elif has_jsonb:
            query_type = QueryType.COMPLEX
            complexity = QueryComplexity.HIGH
        else:
            query_type = QueryType.SIMPLE
            complexity = QueryComplexity.LOW
        
        return QueryAnalysis(
            query_type=query_type,
            complexity=complexity,
            required_fields=["product_code", "name"],
            jsonb_fields=["risk_json"] if has_jsonb else [],
            filters=["åŸºäºå…³é”®è¯æ¨æ–­"],
            aggregations=[],
            confidence=0.6,
            reasoning=f"LLMåˆ†æå¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…é™çº§åˆ†æã€‚é”™è¯¯: {error}",
            suggested_strategy="ä½¿ç”¨æ ‡å‡†æŸ¥è¯¢æ¨¡æ¿"
        )

class QueryOptimizer:
    """åŸºäºè·¯ç”±ç»“æœçš„æŸ¥è¯¢ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.optimization_strategies = {
            QueryType.SIMPLE: self._optimize_simple_query,
            QueryType.COMPLEX: self._optimize_complex_query,
            QueryType.HYBRID: self._optimize_hybrid_query,
            QueryType.AGGREGATION: self._optimize_aggregation_query,
            QueryType.ANALYTICAL: self._optimize_analytical_query
        }
    
    def optimize_query_strategy(self, analysis: QueryAnalysis) -> Dict[str, Any]:
        """æ ¹æ®åˆ†æç»“æœä¼˜åŒ–æŸ¥è¯¢ç­–ç•¥"""
        optimizer = self.optimization_strategies.get(analysis.query_type)
        if optimizer:
            return optimizer(analysis)
        else:
            return self._default_optimization(analysis)
    
    def _optimize_simple_query(self, analysis: QueryAnalysis) -> Dict[str, Any]:
        """ä¼˜åŒ–ç®€å•æŸ¥è¯¢"""
        return {
            "strategy": "direct_scalar",
            "use_indexes": ["risk_level", "currency", "allow_buy"],
            "limit_early": True,
            "avoid_jsonb": True,
            "estimated_performance": "fast"
        }
    
    def _optimize_complex_query(self, analysis: QueryAnalysis) -> Dict[str, Any]:
        """ä¼˜åŒ–å¤æ‚JSONBæŸ¥è¯¢"""
        return {
            "strategy": "jsonb_focused",
            "use_gin_indexes": analysis.jsonb_fields,
            "lateral_joins": True,
            "filter_before_expand": True,
            "estimated_performance": "medium"
        }
    
    def _optimize_hybrid_query(self, analysis: QueryAnalysis) -> Dict[str, Any]:
        """ä¼˜åŒ–æ··åˆæŸ¥è¯¢"""
        return {
            "strategy": "scalar_first_jsonb_second",
            "filter_order": ["scalar_filters", "jsonb_expansion"],
            "use_both_indexes": True,
            "estimated_performance": "medium"
        }
    
    def _optimize_aggregation_query(self, analysis: QueryAnalysis) -> Dict[str, Any]:
        """ä¼˜åŒ–èšåˆæŸ¥è¯¢"""
        return {
            "strategy": "aggregation_optimized",
            "group_early": True,
            "use_partial_indexes": True,
            "estimated_performance": "slow"
        }
    
    def _optimize_analytical_query(self, analysis: QueryAnalysis) -> Dict[str, Any]:
        """ä¼˜åŒ–åˆ†ææŸ¥è¯¢"""
        return {
            "strategy": "analytical_complex",
            "consider_materialized_views": True,
            "parallel_processing": True,
            "estimated_performance": "very_slow"
        }
    
    def _default_optimization(self, analysis: QueryAnalysis) -> Dict[str, Any]:
        """é»˜è®¤ä¼˜åŒ–ç­–ç•¥"""
        return {
            "strategy": "default",
            "estimated_performance": "unknown"
        }

def demo_query_router():
    """æ¼”ç¤ºæŸ¥è¯¢è·¯ç”±å™¨åŠŸèƒ½"""
    print("ğŸ¯ LLM æŸ¥è¯¢è·¯ç”±å™¨æ¼”ç¤º")
    print("=" * 50)
    
    # åˆå§‹åŒ–è·¯ç”±å™¨
    router = LLMQueryRouter()
    optimizer = QueryOptimizer()
    
    # æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
    test_queries = [
        "æ‰¾å‡ºé£é™©ç­‰çº§ä¸º5çš„ç¾å…ƒåŸºé‡‘",
        "å“ªäº›åŸºé‡‘çš„1å¹´æ”¶ç›Šç‡è¶…è¿‡40%ï¼Ÿ",
        "BlackRockæœ‰å¤šå°‘åªåŸºé‡‘ï¼Ÿå¹³å‡é£é™©ç­‰çº§æ˜¯å¤šå°‘ï¼Ÿ",
        "æ‰¾å‡ºæŒæœ‰è‹¹æœå…¬å¸è‚¡ç¥¨çš„åŸºé‡‘",
        "æ˜¾ç¤ºæ‰€æœ‰åŸºé‡‘çš„èµ„äº§é…ç½®åˆ†å¸ƒæƒ…å†µ",
        "åˆ†æè¿‡å»ä¸€å¹´è¡¨ç°æœ€å¥½çš„åŸºé‡‘çš„å…±åŒç‰¹å¾",
        "æ‰¾å‡ºé£é™©ç­‰çº§3-4ä¸”æŒæœ‰ä¸­å›½è‚¡ç¥¨çš„HSBCåŸºé‡‘çš„1å¹´æ”¶ç›Šç‡"
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nğŸ“ æµ‹è¯•æŸ¥è¯¢ {i}: {query}")
        print("-" * 50)
        
        start_time = time.time()
        
        # åˆ†ææŸ¥è¯¢
        analysis = router.analyze_query(query)
        
        # ä¼˜åŒ–ç­–ç•¥
        optimization = optimizer.optimize_query_strategy(analysis)
        
        analysis_time = time.time() - start_time
        
        # æ˜¾ç¤ºç»“æœ
        print(f"ğŸ¯ æŸ¥è¯¢ç±»å‹: {analysis.query_type.value}")
        print(f"ğŸ“Š å¤æ‚åº¦: {analysis.complexity.value}")
        print(f"ğŸ” æ¶‰åŠå­—æ®µ: {', '.join(analysis.required_fields)}")
        if analysis.jsonb_fields:
            print(f"ğŸ“‹ JSONBå­—æ®µ: {', '.join(analysis.jsonb_fields)}")
        print(f"ğŸ² ç½®ä¿¡åº¦: {analysis.confidence:.2f}")
        print(f"ğŸ’¡ ç­–ç•¥: {optimization['strategy']}")
        print(f"âš¡ é¢„æœŸæ€§èƒ½: {optimization['estimated_performance']}")
        print(f"â±ï¸  åˆ†æè€—æ—¶: {analysis_time:.3f}s")
        print(f"ğŸ“ ç†ç”±: {analysis.reasoning}")

if __name__ == "__main__":
    import os
    
    # æ£€æŸ¥API Key
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        exit(1)
    
    demo_query_router()
