import asyncio
from pathlib import Path
import json
import csv
import random
from datetime import datetime
from typing import List, Dict, Any, Tuple

from playwright.async_api import async_playwright, Request, Response

# Generate timestamp for this run
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# Output paths relative to project root of this submodule
BASE_DIR = Path(__file__).resolve().parents[1]
DATA_DIR = BASE_DIR / f"data_{timestamp}"
DATA_DIR.mkdir(parents=True, exist_ok=True)

# åŸºé‡‘è¯¦æƒ…é¡µé¢URLæ¨¡æ¿
FUND_DETAIL_URL_TEMPLATE = "https://investments3.personal-banking.hsbc.com.hk/public/utb/en-gb/fundDetail/{product_code}"

# é»˜è®¤æµ‹è¯•çš„äº§å“ä»£ç 
DEFAULT_PRODUCT_CODE = "U43051"

# ç½‘ç»œè¯·æ±‚å­˜å‚¨
network_requests: List[Dict[str, Any]] = []
# å¿…é¡»æ•è·çš„8ä¸ªæ ¸å¿ƒAPIï¼ˆå®Œæ•´æ€§æ ¡éªŒç”¨ï¼‰
REQUIRED_APIS = {
    "amh_ut_product",
    "wmds_quoteDetail",
    "wmds_fundQuoteSummary",
    "wmds_holdingAllocation",
    "wmds_topTenHoldings",
    "wmds_otherFundClasses",
    "wmds_fundSearchCriteria",
    "wmds_advanceChart",
}

network_responses: List[Dict[str, Any]] = []

def read_product_codes_csv(csv_file_path: str) -> List[Tuple[str, int]]:
    """è¯»å–äº§å“ä»£ç CSVæ–‡ä»¶"""
    product_codes = []
    csv_path = Path(csv_file_path)

    if not csv_path.exists():
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")
        return product_codes

    try:
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                product_code = row.get('Product_Code', '').strip()
                index = int(row.get('Index', 0))
                if product_code:
                    product_codes.append((product_code, index))

        print(f"âœ… æˆåŠŸè¯»å–CSVæ–‡ä»¶: {len(product_codes)}ä¸ªäº§å“ä»£ç ")
        return product_codes

    except Exception as e:
        print(f"âŒ è¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return product_codes

def should_capture_request(request: Request) -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ•è·è¯¥è¯·æ±‚"""
    url = request.url.lower()
    resource_type = request.resource_type

    # è¿‡æ»¤æ‰é™æ€èµ„æº
    static_extensions = ['.js', '.css', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico',
                        '.woff', '.woff2', '.ttf', '.otf', '.eot', '.webp', '.bmp', '.html']

    # è¿‡æ»¤æ‰é™æ€èµ„æºç±»å‹
    static_resource_types = ['stylesheet', 'script', 'image', 'font', 'media', 'document']

    # è¿‡æ»¤æ‰åŒ…å«é™æ€èµ„æºå…³é”®è¯çš„URL
    static_keywords = ['/css/', '/js/', '/fonts/', '/images/', '/img/', '/static/',
                      'stylesheet', 'javascript', '.min.js', '.min.css', 'bundle.js',
                      'bundle.css', 'jquery', 'bootstrap', 'fontawesome']

    # è¿‡æ»¤æ‰ç‰¹å®šçš„APIç«¯ç‚¹å’Œç¬¬ä¸‰æ–¹æœåŠ¡
    excluded_apis = [
        '/amh/ut/sdktoken',
        'funddetail/',  # ä¸»é¡µé¢è¯·æ±‚
        'logx.optimizely.com',  # Optimizelyæ—¥å¿—
        'lpcdn-vip.lpsnmedia.net',  # LivePersonå®¢æœ
        'match.adsrvr.org',  # å¹¿å‘ŠåŒ¹é…
        'quotespeed.morningstar.com',  # æ™¨æ˜ŸæœåŠ¡
        'sp.analytics.yahoo.com',  # Yahooåˆ†æ
        'td.doubleclick.net',  # Google DoubleClick
        'www.google.com/ccm',  # Google CCM
        'www.googletagmanager.com',  # Google Tag Manager
        'www.facebook.com/tr',  # Facebookåƒç´ 
        'visitor-service-ap-northeast-1.tealiumiq.com',  # Tealiumè®¿å®¢æœåŠ¡
        'googleads.g.doubleclick.net',  # Googleå¹¿å‘Š
        'google.com/ccm/form-data',  # Googleè¡¨å•æ•°æ®
        'bat.bing.com',  # Bingå¹¿å‘Š
        'cm.g.doubleclick.net',  # DoubleClick CookieåŒ¹é…
        'connect.facebook.net',  # Facebookè¿æ¥
        'accdn-vip.lpsnmedia.net',  # LivePerson CDN
        'a19069622224.cdn.optimizely.com'  # Optimizely CDN
    ]

    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
    for ext in static_extensions:
        if ext in url:
            return False

    # æ£€æŸ¥èµ„æºç±»å‹
    if resource_type in static_resource_types:
        return False

    # æ£€æŸ¥URLå…³é”®è¯
    for keyword in static_keywords:
        if keyword in url:
            return False

    # æ£€æŸ¥æ’é™¤çš„APIç«¯ç‚¹å’ŒåŸŸå
    for api in excluded_apis:
        if api in url:
            return False

    # åªä¿ç•™HSBCæ ¸å¿ƒAPIå’Œæ™¨æ˜Ÿæ•°æ®API
    hsbc_api_patterns = [
        'investments3.personal-banking.hsbc.com.hk/shp/wealth-mobile-',
        'quotespeed.morningstar.com/ra/'
    ]

    # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬éœ€è¦çš„API
    for pattern in hsbc_api_patterns:
        if pattern in url:
            return True

    # é»˜è®¤ä¸æ•è·å…¶ä»–è¯·æ±‚
    return False

async def capture_request(request: Request):
    """æ•è·HTTPè¯·æ±‚ï¼ˆè¿‡æ»¤é™æ€èµ„æºï¼‰"""
    try:
        # è¿‡æ»¤é™æ€èµ„æº
        if not should_capture_request(request):
            return

        request_data = {
            "timestamp": datetime.now().isoformat(),
            "method": request.method,
            "url": request.url,
            "headers": dict(request.headers),
            "resource_type": request.resource_type,
            "post_data": request.post_data if request.post_data else None,
            "is_navigation_request": request.is_navigation_request()
        }
        network_requests.append(request_data)
        print(f"ğŸ“¤ è¯·æ±‚: {request.method} {request.url}")
    except Exception as e:
        print(f"âš ï¸  æ•è·è¯·æ±‚æ—¶å‡ºé”™: {e}")

def should_capture_response(response: Response) -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ•è·è¯¥å“åº”ï¼ˆä¸è¯·æ±‚è¿‡æ»¤é€»è¾‘ä¸€è‡´ï¼‰"""
    url = response.url.lower()

    # è¿‡æ»¤æ‰é™æ€èµ„æº
    static_extensions = ['.js', '.css', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico',
                        '.woff', '.woff2', '.ttf', '.otf', '.eot', '.webp', '.bmp', '.html']

    # è¿‡æ»¤æ‰åŒ…å«é™æ€èµ„æºå…³é”®è¯çš„URL
    static_keywords = ['/css/', '/js/', '/fonts/', '/images/', '/img/', '/static/',
                      'stylesheet', 'javascript', '.min.js', '.min.css', 'bundle.js',
                      'bundle.css', 'jquery', 'bootstrap', 'fontawesome']

    # è¿‡æ»¤æ‰ç‰¹å®šçš„APIç«¯ç‚¹å’Œç¬¬ä¸‰æ–¹æœåŠ¡
    excluded_apis = [
        '/amh/ut/sdktoken',
        'funddetail/',  # ä¸»é¡µé¢è¯·æ±‚
        'logx.optimizely.com',  # Optimizelyæ—¥å¿—
        'lpcdn-vip.lpsnmedia.net',  # LivePersonå®¢æœ
        'match.adsrvr.org',  # å¹¿å‘ŠåŒ¹é…
        'quotespeed.morningstar.com',  # æ™¨æ˜ŸæœåŠ¡
        'sp.analytics.yahoo.com',  # Yahooåˆ†æ
        'td.doubleclick.net',  # Google DoubleClick
        'www.google.com/ccm',  # Google CCM
        'www.googletagmanager.com',  # Google Tag Manager
        'www.facebook.com/tr',  # Facebookåƒç´ 
        'visitor-service-ap-northeast-1.tealiumiq.com',  # Tealiumè®¿å®¢æœåŠ¡
        'googleads.g.doubleclick.net',  # Googleå¹¿å‘Š
        'google.com/ccm/form-data',  # Googleè¡¨å•æ•°æ®
        'bat.bing.com',  # Bingå¹¿å‘Š
        'cm.g.doubleclick.net',  # DoubleClick CookieåŒ¹é…
        'connect.facebook.net',  # Facebookè¿æ¥
        'accdn-vip.lpsnmedia.net',  # LivePerson CDN
        'a19069622224.cdn.optimizely.com'  # Optimizely CDN
    ]

    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
    for ext in static_extensions:
        if ext in url:
            return False

    # æ£€æŸ¥URLå…³é”®è¯
    for keyword in static_keywords:
        if keyword in url:
            return False

    # æ£€æŸ¥æ’é™¤çš„APIç«¯ç‚¹å’ŒåŸŸå
    for api in excluded_apis:
        if api in url:
            return False

    # åªä¿ç•™HSBCæ ¸å¿ƒAPIå’Œæ™¨æ˜Ÿæ•°æ®API
    hsbc_api_patterns = [
        'investments3.personal-banking.hsbc.com.hk/shp/wealth-mobile-',
        'quotespeed.morningstar.com/ra/'
    ]

    # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬éœ€è¦çš„API
    for pattern in hsbc_api_patterns:
        if pattern in url:
            return True

    # é»˜è®¤ä¸æ•è·å…¶ä»–å“åº”
    return False

async def capture_response(response: Response):
    """æ•è·HTTPå“åº”ï¼ˆè¿‡æ»¤é™æ€èµ„æºï¼‰"""
    try:
        # è¿‡æ»¤é™æ€èµ„æº
        if not should_capture_response(response):
            return

        # è·å–å“åº”ä½“ï¼ˆä»…å¯¹ç‰¹å®šç±»å‹ï¼‰
        response_body = None
        content_type = response.headers.get("content-type", "")

        if any(t in content_type.lower() for t in ["json", "text", "xml", "html"]):
            try:
                response_body = await response.text()
            except:
                response_body = None

        response_data = {
            "timestamp": datetime.now().isoformat(),
            "url": response.url,
            "status": response.status,
            "status_text": response.status_text,
            "headers": dict(response.headers),
            "content_type": content_type,
            "response_body": response_body,
            "ok": response.ok,
            "from_service_worker": response.from_service_worker
        }
        network_responses.append(response_data)
        print(f"ğŸ“¥ å“åº”: {response.status} {response.url}")
    except Exception as e:
        print(f"âš ï¸  æ•è·å“åº”æ—¶å‡ºé”™: {e}")

def extract_api_name(url: str) -> str:
    """ä»URLä¸­æå–APIåç§°"""
    # æŸ¥æ‰¾HSBC APIè·¯å¾„
    if '/shp/wealth-mobile-' in url:
        # æå–APIè·¯å¾„çš„æœ€åéƒ¨åˆ†
        parts = url.split('/')
        for i, part in enumerate(parts):
            if part.startswith('v0') and i + 1 < len(parts):
                # è·å–v0åé¢çš„è·¯å¾„éƒ¨åˆ†ï¼Œç”¨ä¸‹åˆ’çº¿è¿æ¥
                api_parts = parts[i+1:]
                # ç§»é™¤æŸ¥è¯¢å‚æ•°
                if '?' in api_parts[-1]:
                    api_parts[-1] = api_parts[-1].split('?')[0]
                return '_'.join(api_parts)

    # å¯¹äºå…¶ä»–URLï¼Œå°è¯•æå–æœ‰æ„ä¹‰çš„éƒ¨åˆ†
    if 'morningstar.com' in url:
        if '/ra/' in url:
            path_part = url.split('/ra/')[-1].split('?')[0]
            return f"morningstar_{path_part}"

    # é»˜è®¤æƒ…å†µï¼Œä½¿ç”¨åŸŸåå’Œè·¯å¾„
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc.split('.')[0]  # å–åŸŸåç¬¬ä¸€éƒ¨åˆ†
        path = parsed.path.strip('/').replace('/', '_')
        if path:
            return f"{domain}_{path}"
        else:
            return domain
    except:
        return "unknown_api"

async def save_network_data(product_code: str, item_number: int, task_requests: List[Dict[str, Any]], task_responses: List[Dict[str, Any]]):
    """ä¿å­˜ç½‘ç»œæ•°æ®åˆ°ä¸´æ—¶ç›®å½•å¹¶åŸå­è½ç›˜åˆ°æœ€ç»ˆç›®å½•ï¼Œç¡®ä¿åªåœ¨å®Œæˆæ—¶ç”Ÿæˆè¾“å‡º"""

    # ç›®å½•: ä½¿ç”¨ç°æœ‰çš„ .NNN_Code.working ç›®å½•ï¼ˆç”±è°ƒåº¦å™¨/workeråˆ›å»ºï¼‰
    working_dir = DATA_DIR / f".{item_number:03d}_{product_code}.working"
    working_dir.mkdir(parents=True, exist_ok=True)
    # æœ€ç»ˆç›®å½•ç”±å¤–å±‚ complete_work æ§åˆ¶åˆ‡æ¢ä¸º .done


    # æŒ‰APIåˆ†ç»„ä¿å­˜è¯·æ±‚å’Œå“åº”
    api_groups: Dict[str, Dict[str, List[Dict[str, Any]]]] = {}

    for req in task_requests:
        api_name = extract_api_name(req["url"])
        api_groups.setdefault(api_name, {"requests": [], "responses": []})
        api_groups[api_name]["requests"].append(req)

    for resp in task_responses:
        api_name = extract_api_name(resp["url"])
        api_groups.setdefault(api_name, {"requests": [], "responses": []})
        api_groups[api_name]["responses"].append(resp)

    saved_files = []

    # ä¸ºæ¯ä¸ªAPIä¿å­˜å•ç‹¬çš„è¯·æ±‚å’Œå“åº”æ–‡ä»¶ï¼ˆå†™åˆ°å·¥ä½œç›®å½•ï¼‰
    for api_name, data in api_groups.items():
        if data["requests"]:
            request_file = working_dir / f"{api_name}.request.json"
            with open(request_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "api_name": api_name,
                    "product_code": product_code,
                    "timestamp": timestamp,
                    "total_requests": len(data["requests"]),
                    "requests": data["requests"]
                }, f, indent=2, ensure_ascii=False)
            saved_files.append(request_file)

        if data["responses"]:
            response_file = working_dir / f"{api_name}.response.json"
            with open(response_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "api_name": api_name,
                    "product_code": product_code,
                    "timestamp": timestamp,
                    "total_responses": len(data["responses"]),
                    "responses": data["responses"]
                }, f, indent=2, ensure_ascii=False)
            saved_files.append(response_file)

    # ä¿å­˜æ±‡æ€»ä¿¡æ¯ï¼ˆå†™åˆ°å·¥ä½œç›®å½•ï¼‰
    summary_file = working_dir / "summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump({
            "product_code": product_code,
            "timestamp": timestamp,
            "session_dir": str(working_dir),
            "summary": {
                "total_requests": len(task_requests),
                "total_responses": len(task_responses),
                "api_count": len(api_groups),
                "api_names": list(api_groups.keys()),
                "unique_domains": list(set([req["url"].split("/")[2] for req in task_requests if "url" in req])),
                "request_methods": list(set([req["method"] for req in task_requests if "method" in req])),
                "response_status_codes": list(set([resp["status"] for resp in task_responses if "status" in resp]))
            }
        }, f, indent=2, ensure_ascii=False)
    saved_files.append(summary_file)

    # ç§»åŠ¨æˆªå›¾æ–‡ä»¶åˆ°å·¥ä½œç›®å½•
    temp_screenshot_file = DATA_DIR / f"fund_detail_{product_code}_{timestamp}.png"
    if temp_screenshot_file.exists():
        final_screenshot_file = working_dir / f"fund_detail_{product_code}_{timestamp}.png"
        temp_screenshot_file.rename(final_screenshot_file)
        saved_files.append(final_screenshot_file)
        print(f"ğŸ“¸ æˆªå›¾å·²ç§»åŠ¨åˆ°å·¥ä½œç›®å½•: {final_screenshot_file}")

    # å®Œæ•´æ€§æ ¡éªŒï¼šè¦æ±‚8ä¸ªæ ¸å¿ƒAPIå…¨éƒ¨å­˜åœ¨ä¸”æ¯ä¸ªè‡³å°‘æœ‰1ä¸ªå“åº”
    found_api_names = set(api_groups.keys())
    missing = list(REQUIRED_APIS - found_api_names)
    has_all_required = len(missing) == 0 and all(len(api_groups.get(api, {}).get("responses", [])) > 0 for api in REQUIRED_APIS)

    if not has_all_required:
        # ä¸è½ç›˜æœ€ç»ˆç›®å½•ï¼Œä¿ç•™å·¥ä½œç›®å½•ç”¨äºè¯Šæ–­ï¼Œå¹¶è¿”å›å¤±è´¥æ ‡è®°
        print(f"âŒ å®Œæ•´æ€§æ ¡éªŒå¤±è´¥ï¼šç¼ºå°‘API {missing} æˆ–æŸäº›APIæ— å“åº”ã€‚ä¿ç•™å·¥ä½œç›®å½•: {working_dir}")
        final_effective = None
        return {
            "session_dir": final_effective,
            "saved_files": saved_files,
            "api_groups": api_groups,
            "complete": False,
            "missing": missing,
        }

    # åœ¨å¹¶å‘é˜Ÿåˆ—æ¨¡å¼ä¸‹ï¼Œä¸åœ¨æ­¤å¤„é‡å‘½åæœ€ç»ˆç›®å½•ï¼›ç”± complete_work ç»Ÿä¸€åˆ‡æ¢ä¸º .done
    final_effective = working_dir

    print(f"\nğŸ’¾ ç½‘ç»œæ•°æ®å·²å†™å…¥å·¥ä½œç›®å½•: {final_effective}")
    print(f"ğŸ“Š APIåˆ†ç»„æ•°é‡: {len(api_groups)}")
    for api_name in sorted(api_groups.keys()):
        req_count = len(api_groups[api_name]["requests"])
        resp_count = len(api_groups[api_name]["responses"])
        print(f"   ğŸ“ {api_name}: {req_count}ä¸ªè¯·æ±‚, {resp_count}ä¸ªå“åº”")

    return {
        "session_dir": final_effective,
        "saved_files": saved_files,
        "api_groups": api_groups,
        "complete": True
    }

async def main(product_code: str = DEFAULT_PRODUCT_CODE, item_number: int = 1, headless: bool = True):
    """ä¸»å‡½æ•°ï¼šè®¿é—®åŸºé‡‘è¯¦æƒ…é¡µé¢å¹¶æ•è·ç½‘ç»œè¯·æ±‚"""

    # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„ç½‘ç»œè¯·æ±‚å­˜å‚¨
    task_network_requests: List[Dict[str, Any]] = []
    task_network_responses: List[Dict[str, Any]] = []

    async def capture_request(request: Request):
        """æ•è·HTTPè¯·æ±‚ï¼ˆä»»åŠ¡çº§åˆ«ï¼‰"""
        try:
            # è¿‡æ»¤é™æ€èµ„æº
            if not should_capture_request(request):
                return

            request_data = {
                "timestamp": datetime.now().isoformat(),
                "method": request.method,
                "url": request.url,
                "headers": dict(request.headers),
                "resource_type": request.resource_type,
                "post_data": request.post_data if request.post_data else None,
                "is_navigation_request": request.is_navigation_request()
            }
            task_network_requests.append(request_data)
            print(f"ğŸ“¤ [{product_code}] è¯·æ±‚: {request.method} {request.url}")
        except Exception as e:
            print(f"âš ï¸  [{product_code}] æ•è·è¯·æ±‚æ—¶å‡ºé”™: {e}")

    async def capture_response(response: Response):
        """æ•è·HTTPå“åº”ï¼ˆä»»åŠ¡çº§åˆ«ï¼‰"""
        try:
            # è¿‡æ»¤é™æ€èµ„æº
            if not should_capture_response(response):
                return

            # è·å–å“åº”ä½“ï¼ˆä»…å¯¹ç‰¹å®šç±»å‹ï¼‰
            response_body = None
            content_type = response.headers.get("content-type", "")

            if any(t in content_type.lower() for t in ["json", "text", "xml", "html"]):
                try:
                    response_body = await response.text()
                except:
                    response_body = None

            response_data = {
                "timestamp": datetime.now().isoformat(),
                "url": response.url,
                "status": response.status,
                "status_text": response.status_text,
                "headers": dict(response.headers),
                "content_type": content_type,
                "response_body": response_body,
                "ok": response.ok,
                "from_service_worker": response.from_service_worker
            }
            task_network_responses.append(response_data)
            print(f"ğŸ“¥ [{product_code}] å“åº”: {response.status} {response.url}")
        except Exception as e:
            print(f"âš ï¸  [{product_code}] æ•è·å“åº”æ—¶å‡ºé”™: {e}")

    fund_detail_url = FUND_DETAIL_URL_TEMPLATE.format(product_code=product_code)

    print("="*60)
    print(f"ğŸš€ å¼€å§‹æ•è·åŸºé‡‘è¯¦æƒ…é¡µé¢ç½‘ç»œè¯·æ±‚")
    print(f"ğŸ“‹ äº§å“ä»£ç : {product_code}")
    print(f"ğŸŒ ç›®æ ‡URL: {fund_detail_url}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {DATA_DIR}")
    print("="*60)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=headless)
        context = await browser.new_context()
        page = await context.new_page()

        # è®¾ç½®ç½‘ç»œç›‘å¬å™¨ï¼ˆä½¿ç”¨ä»»åŠ¡çº§åˆ«çš„æ•è·å‡½æ•°ï¼‰
        page.on("request", capture_request)
        page.on("response", capture_response)

        print(f"\n=== æ­¥éª¤ 1: è®¿é—®åŸºé‡‘è¯¦æƒ…é¡µé¢ ===")
        print(f"ğŸŒ æ­£åœ¨è®¿é—®: {fund_detail_url}")

        try:
            # è®¿é—®åŸºé‡‘è¯¦æƒ…é¡µé¢
            await page.goto(fund_detail_url, wait_until="domcontentloaded", timeout=60000)

            # ç­‰å¾…ç½‘ç»œè¯·æ±‚å®Œæˆ
            print("â³ ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ...")
            try:
                await page.wait_for_load_state("networkidle", timeout=30000)
            except:
                print("âš ï¸  ç½‘ç»œç©ºé—²ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­æ‰§è¡Œ")
                await page.wait_for_timeout(5000)

            # éªŒè¯é¡µé¢åŠ è½½æˆåŠŸ
            page_title = await page.title()
            print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {page_title}")

            # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«åŸºé‡‘ç›¸å…³å†…å®¹
            if product_code in page_title or "fund" in page_title.lower() or "åŸºé‡‘" in page_title:
                print("âœ… é¡µé¢åŠ è½½æˆåŠŸï¼ŒåŒ…å«åŸºé‡‘ç›¸å…³å†…å®¹")
            else:
                print("âš ï¸  é¡µé¢æ ‡é¢˜æœªåŒ…å«é¢„æœŸçš„åŸºé‡‘ä¿¡æ¯")

            # ç­‰å¾…é¢å¤–æ—¶é—´ç¡®ä¿æ‰€æœ‰å¼‚æ­¥è¯·æ±‚å®Œæˆ
            print("â³ ç­‰å¾…å¼‚æ­¥è¯·æ±‚å®Œæˆ...")
            await page.wait_for_timeout(10000)

            # æ‹æ‘„é¡µé¢æˆªå›¾ - å…ˆåˆ›å»ºä¸´æ—¶æ–‡ä»¶ï¼Œç¨åç§»åŠ¨åˆ°ä¼šè¯ç›®å½•
            temp_screenshot_file = DATA_DIR / f"fund_detail_{product_code}_{timestamp}.png"
            await page.screenshot(path=str(temp_screenshot_file), full_page=True, scale="css")
            print(f"ğŸ“¸ é¡µé¢æˆªå›¾å·²ä¿å­˜: {temp_screenshot_file}")

        except Exception as e:
            print(f"âŒ è®¿é—®é¡µé¢æ—¶å‡ºé”™: {e}")

        print(f"\n=== æ­¥éª¤ 2: ä¿å­˜ç½‘ç»œæ•°æ® ===")
        result = await save_network_data(product_code, item_number, task_network_requests, task_network_responses)

        print(f"\nğŸ“Š ç½‘ç»œè¯·æ±‚ç»Ÿè®¡:")
        print(f"   ğŸ“¤ æ€»è¯·æ±‚æ•°: {len(task_network_requests)}")
        print(f"   ğŸ“¥ æ€»å“åº”æ•°: {len(task_network_responses)}")

        if task_network_requests:
            domains = set([req["url"].split("/")[2] for req in task_network_requests if "url" in req])
            methods = set([req["method"] for req in task_network_requests if "method" in req])
            print(f"   ğŸŒ æ¶‰åŠåŸŸå: {len(domains)} ä¸ª")
            print(f"   ğŸ”§ è¯·æ±‚æ–¹æ³•: {', '.join(methods)}")

        if task_network_responses:
            status_codes = set([resp["status"] for resp in task_network_responses if "status" in resp])
            print(f"   ğŸ“Š å“åº”çŠ¶æ€ç : {', '.join(map(str, status_codes))}")

        await context.close()
        await browser.close()

        if result and result.get("complete", True) is False:
            # æ ‡è®°æœªå®Œæˆï¼ŒæŠ›å‡ºå¼‚å¸¸è®©ä¸Šå±‚å¤„ç†ï¼ˆå¹¶å‘æ± è®°å½•ä¸ºå¤±è´¥ï¼Œä¸äº§å‡ºæœ€ç»ˆç›®å½•ï¼‰
            missing = result.get("missing")
            raise RuntimeError(f"å®Œæ•´æ€§æ ¡éªŒå¤±è´¥ï¼Œç¼ºå°‘API: {missing}")

        print(f"\nğŸ¯ ä»»åŠ¡å®Œæˆï¼æ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ°: {DATA_DIR}")
        return result

async def process_single_product(product_code: str, item_number: int, headless: bool = True):
    """å¤„ç†å•ä¸ªäº§å“ä»£ç """
    print(f"\n{'='*60}")
    print(f"ğŸ¯ å¤„ç†äº§å“ #{item_number}: {product_code}")
    print(f"{'='*60}")

    try:
        await main(product_code=product_code, item_number=item_number, headless=headless)
        print(f"âœ… äº§å“ {product_code} å¤„ç†å®Œæˆ")
        return True
    except Exception as e:
        print(f"âŒ äº§å“ {product_code} å¤„ç†å¤±è´¥: {e}")
        return False

async def initialize_work_queue(products: List[Tuple[str, int]]):
    """åˆå§‹åŒ–å·¥ä½œé˜Ÿåˆ—ï¼šä¸ºæ‰€æœ‰äº§å“åˆ›å»º .init çŠ¶æ€ç›®å½•"""
    print(f"\nğŸ”§ åˆå§‹åŒ–å·¥ä½œé˜Ÿåˆ—: {len(products)} ä¸ªäº§å“")

    for product_code, item_number in products:
        init_dir = DATA_DIR / f".{item_number:03d}_{product_code}.init"
        init_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºçŠ¶æ€æ–‡ä»¶
        status_file = init_dir / "status.json"
        with open(status_file, 'w', encoding='utf-8') as f:
            json.dump({
                "product_code": product_code,
                "item_number": item_number,
                "status": "init",
                "created_at": datetime.now().isoformat(),
                "retry_count": 0,
                "max_retries": 3
            }, f, indent=2, ensure_ascii=False)

    print(f"âœ… å·¥ä½œé˜Ÿåˆ—åˆå§‹åŒ–å®Œæˆ")

async def claim_next_work() -> Tuple[str, int, Path]:
    """ä¸»åŠ¨è·å–ä¸‹ä¸€ä¸ªå·¥ä½œé¡¹ï¼šåŸå­åœ°å°† .init é‡å‘½åä¸º .working"""
    init_dirs = list(DATA_DIR.glob("*.init"))

    for init_dir in init_dirs:
        # è§£æç›®å½•åè·å–äº§å“ä¿¡æ¯
        dir_name = init_dir.name
        if not dir_name.startswith('.') or not dir_name.endswith('.init'):
            continue

        try:
            # è§£æ .NNN_ProductCode.init
            base_name = dir_name[1:-5]  # å»æ‰å¼€å¤´çš„ . å’Œç»“å°¾çš„ .init
            parts = base_name.split('_', 1)
            if len(parts) != 2:
                continue
            item_number = int(parts[0])
            product_code = parts[1]

            # åŸå­é‡å‘½åä¸º .working
            working_dir = DATA_DIR / f".{item_number:03d}_{product_code}.working"
            try:
                init_dir.rename(working_dir)
                print(f"ğŸ”’ è·å–å·¥ä½œ: #{item_number} {product_code}")
                return product_code, item_number, working_dir
            except FileExistsError:
                # å·²è¢«å…¶ä»–workerè·å–ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª
                continue
        except (ValueError, IndexError):
            # è§£æå¤±è´¥ï¼Œè·³è¿‡
            continue

    # æ²¡æœ‰å¯ç”¨å·¥ä½œ
    return None, None, None

async def complete_work(product_code: str, item_number: int, working_dir: Path, success: bool, error: str = None):
    """å®Œæˆå·¥ä½œï¼šå°† .working é‡å‘½åä¸º .done æˆ– .error"""
    status_file = working_dir / "status.json"

    # è¯»å–å½“å‰çŠ¶æ€
    try:
        with open(status_file, 'r', encoding='utf-8') as f:
            status = json.load(f)
    except:
        status = {"retry_count": 0, "max_retries": 3}

    if success:
        # æˆåŠŸï¼šé‡å‘½åä¸º .done
        done_dir = DATA_DIR / f"{item_number:03d}_{product_code}.done"
        status.update({
            "status": "done",
            "completed_at": datetime.now().isoformat(),
            "success": True
        })
        with open(status_file, 'w', encoding='utf-8') as f:
            json.dump(status, f, indent=2, ensure_ascii=False)

        try:
            working_dir.rename(done_dir)
            print(f"âœ… å®Œæˆå·¥ä½œ: #{item_number} {product_code}")
        except Exception as e:
            print(f"âš ï¸ é‡å‘½åä¸ºdoneå¤±è´¥: {e}")
    else:
        # å¤±è´¥ï¼šæ£€æŸ¥é‡è¯•æ¬¡æ•°
        retry_count = status.get("retry_count", 0)
        max_retries = status.get("max_retries", 3)

        if retry_count < max_retries:
            # é‡è¯•ï¼šé‡å‘½åå› .init
            retry_count += 1
            status.update({
                "status": "init",
                "retry_count": retry_count,
                "last_error": error,
                "last_attempt_at": datetime.now().isoformat()
            })
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump(status, f, indent=2, ensure_ascii=False)

            init_dir = DATA_DIR / f".{item_number:03d}_{product_code}.init"
            try:
                working_dir.rename(init_dir)
                print(f"ğŸ”„ é‡è¯•å·¥ä½œ: #{item_number} {product_code} (ç¬¬{retry_count}æ¬¡)")
            except Exception as e:
                print(f"âš ï¸ é‡å‘½åä¸ºinitå¤±è´¥: {e}")
        else:
            # è¶…è¿‡é‡è¯•æ¬¡æ•°ï¼šé‡å‘½åä¸º .error
            error_dir = DATA_DIR / f"{item_number:03d}_{product_code}.error"
            status.update({
                "status": "error",
                "completed_at": datetime.now().isoformat(),
                "success": False,
                "final_error": error,
                "retry_count": retry_count
            })
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump(status, f, indent=2, ensure_ascii=False)

            try:
                working_dir.rename(error_dir)
                print(f"âŒ é”™è¯¯å·¥ä½œ: #{item_number} {product_code} (é‡è¯•{retry_count}æ¬¡åå¤±è´¥)")
            except Exception as e:
                print(f"âš ï¸ é‡å‘½åä¸ºerrorå¤±è´¥: {e}")

async def worker_process(worker_id: int, headless: bool = True):
    """å·¥ä½œè¿›ç¨‹ï¼šä¸»åŠ¨è·å–å¹¶å¤„ç†å·¥ä½œé¡¹"""
    print(f"ğŸ”§ å¯åŠ¨å·¥ä½œè¿›ç¨‹ #{worker_id}")

    while True:
        # ä¸»åŠ¨è·å–å·¥ä½œ
        product_code, item_number, working_dir = await claim_next_work()

        if product_code is None:
            # æ²¡æœ‰å¯ç”¨å·¥ä½œï¼Œç­‰å¾…ä¸€ä¸‹å†æ£€æŸ¥
            await asyncio.sleep(1)
            continue

        print(f"ğŸ¯ å·¥ä½œè¿›ç¨‹ #{worker_id} å¤„ç†: #{item_number} {product_code}")

        # å¤„ç†å·¥ä½œ
        success = False
        error = None
        try:
            success = await process_single_product(product_code, item_number, headless)
        except Exception as e:
            error = str(e)
            success = False

        # å®Œæˆå·¥ä½œ
        await complete_work(product_code, item_number, working_dir, success, error)

async def process_with_pool(products: List[Tuple[str, int]], pool_size: int, headless: bool = True):
    """ä½¿ç”¨åŸºäºæ–‡ä»¶ç³»ç»ŸçŠ¶æ€çš„å·¥ä½œé˜Ÿåˆ—å¹¶å‘æ± """
    total = len(products)
    print(f"\nğŸš€ å·¥ä½œé˜Ÿåˆ—å¹¶å‘æ± å¯åŠ¨: æ€»è®¡ {total} ä¸ªäº§å“, æ± å¤§å°={pool_size}")

    # åˆå§‹åŒ–å·¥ä½œé˜Ÿåˆ—
    await initialize_work_queue(products)

    # å¯åŠ¨å·¥ä½œè¿›ç¨‹
    workers = []
    for i in range(pool_size):
        worker = asyncio.create_task(worker_process(i + 1, headless))
        workers.append(worker)

    # ç›‘æ§è¿›åº¦
    while True:
        # ç»Ÿè®¡å„çŠ¶æ€çš„æ•°é‡
        init_count = len(list(DATA_DIR.glob("*.init")))
        working_count = len(list(DATA_DIR.glob("*.working")))
        done_count = len(list(DATA_DIR.glob("*.done")))
        error_count = len(list(DATA_DIR.glob("*.error")))

        total_processed = done_count + error_count

        if total_processed >= total:
            # å…¨éƒ¨å®Œæˆï¼Œåœæ­¢å·¥ä½œè¿›ç¨‹
            for worker in workers:
                worker.cancel()
            break

        # æ¯10ç§’æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
        print(f"ğŸ“Š è¿›åº¦: âœ…{done_count} å®Œæˆ, âŒ{error_count} é”™è¯¯, ğŸ”„{working_count} å¤„ç†ä¸­, â³{init_count} ç­‰å¾…")
        await asyncio.sleep(10)

    # ç­‰å¾…æ‰€æœ‰å·¥ä½œè¿›ç¨‹ç»“æŸ
    await asyncio.gather(*workers, return_exceptions=True)

    # ç”Ÿæˆç»“æœ
    results = []
    for status_dir in DATA_DIR.glob("*.done"):
        status_file = status_dir / "status.json"
        if status_file.exists():
            with open(status_file, 'r', encoding='utf-8') as f:
                status = json.load(f)
                results.append({
                    "product_code": status["product_code"],
                    "item_number": status["item_number"],
                    "success": True,
                    "session_dir": str(status_dir)
                })

    for status_dir in DATA_DIR.glob("*.error"):
        status_file = status_dir / "status.json"
        if status_file.exists():
            with open(status_file, 'r', encoding='utf-8') as f:
                status = json.load(f)
                results.append({
                    "product_code": status["product_code"],
                    "item_number": status["item_number"],
                    "success": False,
                    "error": status.get("final_error", "Unknown error"),
                    "session_dir": None
                })

    results.sort(key=lambda x: x["item_number"])
    success_count = sum(1 for r in results if r["success"])
    failed_count = total - success_count

    print(f"\nğŸ¯ å·¥ä½œé˜Ÿåˆ—å¤„ç†å®Œæˆ: âœ…{success_count} æˆåŠŸ, âŒ{failed_count} å¤±è´¥")
    return results

async def process_batch_concurrent(batch_products: List[Tuple[str, int]], headless: bool = True):
    """å¹¶å‘å¤„ç†ä¸€æ‰¹äº§å“ï¼ˆä¾‹å¦‚ 10 ä¸ªï¼‰ï¼Œæ¯ä¸ªä»»åŠ¡éšæœºå»¶è¿Ÿ 1-10 ç§’å¯åŠ¨ï¼Œé¿å…åŒæ—¶å¹¶å‘å°–å³°"""
    print(f"\nğŸ”„ å¹¶å‘å¤„ç†æ‰¹æ¬¡: {len(batch_products)}ä¸ªäº§å“ (éšæœºå¯åŠ¨ 1-10s)")
    for product_code, item_number in batch_products:
        print(f"   ğŸ“‹ #{item_number}: {product_code}")

    async def delayed_start(product_code: str, item_number: int, headless: bool):
        delay = random.randint(1, 10)
        print(f"â³ ä»»åŠ¡é¢„çƒ­: #{item_number} {product_code} å°†åœ¨ {delay}s åå¯åŠ¨")
        await asyncio.sleep(delay)
        return await process_single_product(product_code, item_number, headless)

    # åˆ›å»ºå¹¶å‘ä»»åŠ¡ï¼ˆå¸¦éšæœºå»¶è¿Ÿï¼‰
    tasks = []
    for product_code, item_number in batch_products:
        task = asyncio.create_task(
            delayed_start(product_code, item_number, headless),
            name=f"product_{item_number}_{product_code}"
        )
        tasks.append((task, product_code, item_number))

    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    results = []
    for task, product_code, item_number in tasks:
        try:
            success = await task
            results.append((product_code, item_number, success))
        except Exception as e:
            print(f"âŒ äº§å“ {product_code} å¤„ç†å¼‚å¸¸: {e}")
            results.append((product_code, item_number, False))

    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for _, _, success in results if success)
    failed_count = len(results) - success_count

    print(f"ğŸ¯ æ‰¹æ¬¡å¤„ç†å®Œæˆ: âœ…{success_count}ä¸ªæˆåŠŸ, âŒ{failed_count}ä¸ªå¤±è´¥")
    return results

async def process_csv_file(csv_file_path: str, headless: bool = True, start_index: int = 1, end_index: int = None, pool_size: int = 3):
    """æ‰¹é‡å¤„ç†CSVæ–‡ä»¶ä¸­çš„äº§å“ä»£ç ï¼ˆæ”¯æŒå¹¶å‘ï¼‰"""
    product_codes = read_product_codes_csv(csv_file_path)

    if not product_codes:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„äº§å“ä»£ç ")
        return

    # è¿‡æ»¤ç´¢å¼•èŒƒå›´
    if end_index is None:
        end_index = len(product_codes)

    filtered_products = [(code, idx) for code, idx in product_codes if start_index <= idx <= end_index]

    # å…¼å®¹æ—§æ—¥å¿—å­—æ®µæ‰€éœ€çš„å˜é‡åï¼ˆä»…ç”¨äºæ‰“å°ï¼‰ï¼Œä¸å¹¶å‘æ± å¤§å°ä¿æŒä¸€è‡´
    batch_size = pool_size

    print(f"\nğŸš€ å¼€å§‹å¹¶å‘æ± å¤„ç†:")
    print(f"   ğŸ“ CSVæ–‡ä»¶: {csv_file_path}")
    print(f"   ğŸ“Š æ€»äº§å“æ•°: {len(product_codes)}")
    print(f"   ğŸ¯ å¤„ç†èŒƒå›´: {start_index} - {end_index}")
    print(f"   ğŸ“‹ å®é™…å¤„ç†: {len(filtered_products)}ä¸ªäº§å“")
    print(f"   ï¿½ å¹¶å‘æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   ï¿½ğŸ“ è¾“å‡ºç›®å½•: data_{timestamp}")

    # ä½¿ç”¨å¹¶å‘æ± å¤„ç†
    results = await process_with_pool(filtered_products, pool_size, headless)

    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    success_count = sum(1 for r in results if r["success"])
    failed_count = len(results) - success_count

    report = {
        "timestamp": timestamp,
        "csv_file": csv_file_path,
        "range": {"start_index": start_index, "end_index": end_index},
        "pool_size": pool_size,
        "totals": {"success": success_count, "failed": failed_count},
        "items": results
    }

    report_file = DATA_DIR / "run_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“„ è¿è¡ŒæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    print(f"ğŸ¯ å…¨éƒ¨å®Œæˆ: âœ…{success_count}, âŒ{failed_count}")

if __name__ == "__main__":
    import sys

    # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†CSVæ–‡ä»¶
    if len(sys.argv) > 1 and sys.argv[1].endswith('.csv'):
        # CSVæ‰¹é‡å¤„ç†æ¨¡å¼
        csv_file_path = sys.argv[1]

        # è·å–å¤„ç†èŒƒå›´å‚æ•°
        start_index = int(sys.argv[2]) if len(sys.argv) > 2 else 1
        end_index = int(sys.argv[3]) if len(sys.argv) > 3 else None

        # è·å–å¹¶å‘æ± å¤§å°å‚æ•°
        pool_size = int(sys.argv[4]) if len(sys.argv) > 4 and sys.argv[4].isdigit() else 3

        # è·å–æ˜¯å¦æ— å¤´æ¨¡å¼å‚æ•°
        headless = True
        if len(sys.argv) > 5 and sys.argv[5].lower() in ['false', 'no', '0']:
            headless = False

        print(f"ğŸ¯ CSVå¹¶å‘æ± å¤„ç†æ¨¡å¼:")
        print(f"   ğŸ“ CSVæ–‡ä»¶: {csv_file_path}")
        print(f"   ğŸ“Š å¤„ç†èŒƒå›´: {start_index} - {end_index if end_index else 'æœ«å°¾'}")
        print(f"   ğŸ”„ å¹¶å‘æ± å¤§å°: {pool_size}")
        print(f"   ğŸ–¥ï¸  æ— å¤´æ¨¡å¼: {headless}")

        # è¿è¡Œå¹¶å‘æ± æ‰¹é‡å¤„ç†
        asyncio.run(process_csv_file(csv_file_path, headless, start_index, end_index, pool_size))

    else:
        # å•ä¸ªäº§å“å¤„ç†æ¨¡å¼
        product_code = sys.argv[1] if len(sys.argv) > 1 else DEFAULT_PRODUCT_CODE
        item_number = int(sys.argv[2]) if len(sys.argv) > 2 and sys.argv[2].isdigit() else 1

        # è·å–æ˜¯å¦æ— å¤´æ¨¡å¼å‚æ•°
        headless = True
        if len(sys.argv) > 3 and sys.argv[3].lower() in ['false', 'no', '0']:
            headless = False

        print(f"ğŸ¯ å•ä¸ªäº§å“å¤„ç†æ¨¡å¼:")
        print(f"   ğŸ“‹ äº§å“ä»£ç : {product_code}")
        print(f"   ğŸ”¢ é¡¹ç›®ç¼–å·: {item_number}")
        print(f"   ğŸ–¥ï¸  æ— å¤´æ¨¡å¼: {headless}")
        print(f"   ğŸ“ è¾“å‡ºç›®å½•: data_{timestamp}")

        # è¿è¡Œå•ä¸ªäº§å“å¤„ç†
        asyncio.run(main(product_code=product_code, item_number=item_number, headless=headless))

