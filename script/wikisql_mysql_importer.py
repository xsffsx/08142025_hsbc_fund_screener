#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
WikiSQLå®Œæ•´æ•°æ®å¯¼å…¥MySQLè§£å†³æ–¹æ¡ˆ
æ”¯æŒSQLiteæ•°æ®åº“å’ŒJSONLå…ƒæ•°æ®çš„å®Œæ•´å¯¼å…¥
ä½œè€…: Augment Agent
æ—¥æœŸ: 2025-02-04
"""

import sqlite3
import json
import mysql.connector
from mysql.connector import Error
from pathlib import Path
from tqdm import tqdm
import logging
import re
from typing import Dict, List, Tuple, Optional
import sys

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class WikiSQLMySQLImporter:
    def __init__(self, mysql_config: Dict, wikisql_data_path: str):
        self.mysql_config = mysql_config
        self.wikisql_path = Path(wikisql_data_path)
        self.mysql_conn = None
        
        # æ•°æ®ç±»å‹æ˜ å°„
        self.sqlite_to_mysql_types = {
            'TEXT': 'VARCHAR(500)',
            'INTEGER': 'INT',
            'REAL': 'DECIMAL(15,4)',
            'BLOB': 'LONGBLOB',
            'NUMERIC': 'DECIMAL(15,4)'
        }
        
        # è¯­ä¹‰ä¿¡æ¯ç¼“å­˜
        self.semantic_cache = {}
        
    def connect_mysql(self) -> bool:
        """è¿æ¥MySQLæ•°æ®åº“"""
        try:
            self.mysql_conn = mysql.connector.connect(**self.mysql_config)
            logger.info("âœ… MySQLè¿æ¥æˆåŠŸ")
            return True
        except Error as e:
            logger.error(f"âŒ MySQLè¿æ¥å¤±è´¥: {e}")
            return False
    
    def create_metadata_tables(self) -> bool:
        """åˆ›å»ºå…ƒæ•°æ®è¡¨"""
        if not self.mysql_conn:
            return False
            
        try:
            cursor = self.mysql_conn.cursor()
            
            # 1. è¡¨å…ƒæ•°æ®è¡¨
            create_tables_meta = """
            CREATE TABLE IF NOT EXISTS wikisql_tables_meta (
                id INT AUTO_INCREMENT PRIMARY KEY,
                original_table_id VARCHAR(50) NOT NULL UNIQUE,
                sqlite_table_name VARCHAR(100) NOT NULL,
                mysql_table_name VARCHAR(100) NOT NULL,
                page_title VARCHAR(500),
                section_title VARCHAR(200),
                business_domain VARCHAR(100),
                description TEXT,
                column_count INT DEFAULT 0,
                row_count INT DEFAULT 0,
                data_source ENUM('dev', 'test', 'train') NOT NULL,
                created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE KEY uk_original_table_id (original_table_id),
                INDEX idx_page_title (page_title(100)),
                INDEX idx_data_source (data_source)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='WikiSQLè¡¨å…ƒæ•°æ®'
            """
            
            cursor.execute(create_tables_meta)
            logger.info("âœ… åˆ›å»ºwikisql_tables_metaè¡¨æˆåŠŸ")
            
            # 2. åˆ—å…ƒæ•°æ®è¡¨
            create_columns_meta = """
            CREATE TABLE IF NOT EXISTS wikisql_columns_meta (
                id INT AUTO_INCREMENT PRIMARY KEY,
                table_meta_id INT NOT NULL,
                column_index INT NOT NULL,
                original_column_name VARCHAR(200) NOT NULL,
                mysql_column_name VARCHAR(100) NOT NULL,
                sqlite_column_name VARCHAR(100) NOT NULL,
                data_type VARCHAR(50) NOT NULL,
                mysql_data_type VARCHAR(50) NOT NULL,
                description TEXT,
                sample_values TEXT,
                created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (table_meta_id) REFERENCES wikisql_tables_meta(id) ON DELETE CASCADE,
                INDEX idx_table_meta_id (table_meta_id),
                INDEX idx_column_index (column_index)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='WikiSQLåˆ—å…ƒæ•°æ®'
            """
            
            cursor.execute(create_columns_meta)
            logger.info("âœ… åˆ›å»ºwikisql_columns_metaè¡¨æˆåŠŸ")
            
            # 3. é—®é¢˜æŸ¥è¯¢è¡¨
            create_questions = """
            CREATE TABLE IF NOT EXISTS wikisql_questions (
                id INT AUTO_INCREMENT PRIMARY KEY,
                table_meta_id INT NOT NULL,
                question TEXT NOT NULL,
                sql_select_col INT,
                sql_agg_func INT,
                sql_conditions JSON,
                data_source ENUM('dev', 'test', 'train') NOT NULL,
                created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (table_meta_id) REFERENCES wikisql_tables_meta(id) ON DELETE CASCADE,
                INDEX idx_table_meta_id (table_meta_id),
                INDEX idx_data_source (data_source),
                FULLTEXT KEY ft_question (question)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='WikiSQLé—®é¢˜æŸ¥è¯¢'
            """
            
            cursor.execute(create_questions)
            logger.info("âœ… åˆ›å»ºwikisql_questionsè¡¨æˆåŠŸ")
            
            self.mysql_conn.commit()
            cursor.close()
            return True
            
        except Error as e:
            logger.error(f"âŒ åˆ›å»ºå…ƒæ•°æ®è¡¨å¤±è´¥: {e}")
            return False
    
    def load_semantic_info(self, data_source: str) -> Dict:
        """åŠ è½½è¯­ä¹‰ä¿¡æ¯"""
        semantic_info = {}
        
        tables_jsonl = self.wikisql_path / f"{data_source}.tables.jsonl"
        if not tables_jsonl.exists():
            logger.warning(f"è¡¨ç»“æ„æ–‡ä»¶ä¸å­˜åœ¨: {tables_jsonl}")
            return semantic_info
        
        logger.info(f"ğŸ“– åŠ è½½ {data_source} è¯­ä¹‰ä¿¡æ¯...")
        
        with open(tables_jsonl, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f):
                try:
                    table_data = json.loads(line.strip())
                    table_id = table_data['id']
                    
                    # æå–è¯­ä¹‰ä¿¡æ¯
                    semantic_info[table_id] = {
                        'page_title': table_data.get('page_title', ''),
                        'section_title': table_data.get('section_title', ''),
                        'headers': table_data.get('header', []),
                        'types': table_data.get('types', []),
                        'caption': table_data.get('caption', ''),
                        'name': table_data.get('name', '')
                    }
                    
                except json.JSONDecodeError as e:
                    logger.warning(f"è·³è¿‡æ— æ•ˆJSONè¡Œ {line_num}: {e}")
                    continue
                except Exception as e:
                    logger.error(f"å¤„ç†è¡Œ {line_num} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    continue
        
        logger.info(f"âœ… åŠ è½½äº† {len(semantic_info)} ä¸ªè¡¨çš„è¯­ä¹‰ä¿¡æ¯")
        return semantic_info
    
    def get_sqlite_table_info(self, sqlite_db_path: Path, table_name: str) -> Optional[Dict]:
        """è·å–SQLiteè¡¨ä¿¡æ¯"""
        try:
            conn = sqlite3.connect(sqlite_db_path)
            cursor = conn.cursor()
            
            # è·å–è¡¨ç»“æ„
            cursor.execute(f"PRAGMA table_info({table_name})")
            columns_info = cursor.fetchall()
            
            # è·å–è¡Œæ•°
            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            row_count = cursor.fetchone()[0]
            
            # è·å–ç¤ºä¾‹æ•°æ®
            cursor.execute(f"SELECT * FROM {table_name} LIMIT 3")
            sample_rows = cursor.fetchall()
            
            conn.close()
            
            return {
                'columns': columns_info,
                'row_count': row_count,
                'sample_rows': sample_rows
            }
            
        except Exception as e:
            logger.error(f"è·å–SQLiteè¡¨ä¿¡æ¯å¤±è´¥ {table_name}: {e}")
            return None
    
    def generate_mysql_table_name(self, table_id: str) -> str:
        """ç”ŸæˆMySQLè¡¨å"""
        # å°†ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
        clean_id = re.sub(r'[^a-zA-Z0-9]', '_', table_id)
        return f"wikisql_data_{clean_id}"
    
    def generate_mysql_column_name(self, column_name: str, index: int) -> str:
        """ç”ŸæˆMySQLåˆ—å"""
        if not column_name or column_name.strip() == '':
            return f"col_{index}"
        
        # æ¸…ç†åˆ—å
        clean_name = re.sub(r'[^a-zA-Z0-9]', '_', column_name.lower())
        clean_name = re.sub(r'_+', '_', clean_name).strip('_')
        
        # é¿å…MySQLä¿ç•™å­—
        mysql_reserved = ['order', 'group', 'select', 'from', 'where', 'limit', 'index']
        if clean_name in mysql_reserved:
            clean_name = f"col_{clean_name}"
        
        return clean_name or f"col_{index}"
    
    def create_mysql_table_from_sqlite(self, table_id: str, sqlite_table_name: str, 
                                     sqlite_db_path: Path, semantic_info: Dict) -> Optional[Tuple]:
        """ä»SQLiteè¡¨åˆ›å»ºMySQLè¡¨"""
        
        # è·å–SQLiteè¡¨ä¿¡æ¯
        sqlite_info = self.get_sqlite_table_info(sqlite_db_path, sqlite_table_name)
        if not sqlite_info:
            return None
        
        # ç”ŸæˆMySQLè¡¨å
        mysql_table_name = self.generate_mysql_table_name(table_id)
        
        # è·å–è¯­ä¹‰ä¿¡æ¯
        table_semantic = semantic_info.get(table_id, {})
        headers = table_semantic.get('headers', [])
        types = table_semantic.get('types', [])
        
        try:
            cursor = self.mysql_conn.cursor()
            
            # æ„å»ºåˆ—å®šä¹‰
            column_definitions = []
            column_definitions.append("id INT AUTO_INCREMENT PRIMARY KEY")
            
            columns_meta = []
            
            for i, (cid, name, type_name, notnull, dflt_value, pk) in enumerate(sqlite_info['columns']):
                # è·å–åŸå§‹åˆ—å
                original_name = headers[i] if i < len(headers) else name
                
                # ç”ŸæˆMySQLåˆ—å
                mysql_col_name = self.generate_mysql_column_name(original_name, i)
                
                # è½¬æ¢æ•°æ®ç±»å‹
                mysql_type = self.sqlite_to_mysql_types.get(type_name.upper(), 'VARCHAR(500)')
                
                column_definitions.append(f"{mysql_col_name} {mysql_type}")
                
                # è®°å½•åˆ—å…ƒæ•°æ®
                columns_meta.append({
                    'index': i,
                    'original_name': original_name,
                    'mysql_name': mysql_col_name,
                    'sqlite_name': name,
                    'data_type': type_name,
                    'mysql_type': mysql_type
                })
            
            # æ·»åŠ æ—¶é—´æˆ³åˆ—
            column_definitions.append("created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP")
            
            # åˆ›å»ºè¡¨
            create_table_sql = f"""
            CREATE TABLE IF NOT EXISTS {mysql_table_name} (
                {', '.join(column_definitions)}
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 
            COMMENT='{table_semantic.get("page_title", "")[:100]}'
            """
            
            cursor.execute(create_table_sql)
            logger.info(f"âœ… åˆ›å»ºMySQLè¡¨: {mysql_table_name}")
            
            cursor.close()
            return mysql_table_name, columns_meta
            
        except Error as e:
            logger.error(f"âŒ åˆ›å»ºMySQLè¡¨å¤±è´¥ {mysql_table_name}: {e}")
            return None
    
    def import_table_data(self, sqlite_db_path: Path, sqlite_table_name: str, 
                         mysql_table_name: str, columns_meta: List[Dict]) -> bool:
        """å¯¼å…¥è¡¨æ•°æ®"""
        try:
            # è¿æ¥SQLite
            sqlite_conn = sqlite3.connect(sqlite_db_path)
            sqlite_cursor = sqlite_conn.cursor()
            
            # è·å–æ‰€æœ‰æ•°æ®
            sqlite_cursor.execute(f"SELECT * FROM {sqlite_table_name}")
            rows = sqlite_cursor.fetchall()
            
            if not rows:
                logger.info(f"è¡¨ {sqlite_table_name} æ— æ•°æ®")
                sqlite_conn.close()
                return True
            
            # å‡†å¤‡MySQLæ’å…¥
            mysql_cursor = self.mysql_conn.cursor()
            
            # æ„å»ºæ’å…¥SQL
            mysql_columns = [col['mysql_name'] for col in columns_meta]
            placeholders = ', '.join(['%s'] * len(mysql_columns))
            
            insert_sql = f"""
            INSERT INTO {mysql_table_name} ({', '.join(mysql_columns)})
            VALUES ({placeholders})
            """
            
            # æ‰¹é‡æ’å…¥æ•°æ®
            batch_data = []
            for row in rows:
                # ç¡®ä¿æ•°æ®é•¿åº¦åŒ¹é…
                processed_row = []
                for i, value in enumerate(row):
                    if i < len(columns_meta):
                        # æ•°æ®æ¸…ç†å’Œç±»å‹è½¬æ¢
                        if value is None:
                            processed_row.append(None)
                        else:
                            # é™åˆ¶å­—ç¬¦ä¸²é•¿åº¦
                            str_value = str(value)[:500] if value else None
                            processed_row.append(str_value)
                    
                batch_data.append(tuple(processed_row))
            
            # æ‰§è¡Œæ‰¹é‡æ’å…¥
            mysql_cursor.executemany(insert_sql, batch_data)
            self.mysql_conn.commit()
            
            logger.info(f"âœ… å¯¼å…¥ {len(batch_data)} è¡Œæ•°æ®åˆ° {mysql_table_name}")
            
            sqlite_conn.close()
            mysql_cursor.close()
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¯¼å…¥è¡¨æ•°æ®å¤±è´¥ {mysql_table_name}: {e}")
            return False
    
    def record_table_metadata(self, table_id: str, sqlite_table_name: str, 
                            mysql_table_name: str, semantic_info: Dict, 
                            columns_meta: List[Dict], data_source: str) -> bool:
        """è®°å½•è¡¨å…ƒæ•°æ®"""
        try:
            cursor = self.mysql_conn.cursor()
            
            # æ’å…¥è¡¨å…ƒæ•°æ®
            insert_table_meta = """
            INSERT INTO wikisql_tables_meta 
            (original_table_id, sqlite_table_name, mysql_table_name, page_title, 
             section_title, description, column_count, data_source)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """
            
            page_title = semantic_info.get('page_title', '')
            section_title = semantic_info.get('section_title', '')
            description = f"{page_title} - {section_title}" if section_title else page_title
            
            cursor.execute(insert_table_meta, (
                table_id, sqlite_table_name, mysql_table_name,
                page_title, section_title, description,
                len(columns_meta), data_source
            ))
            
            table_meta_id = cursor.lastrowid
            
            # æ’å…¥åˆ—å…ƒæ•°æ®
            for col_meta in columns_meta:
                insert_column_meta = """
                INSERT INTO wikisql_columns_meta 
                (table_meta_id, column_index, original_column_name, mysql_column_name,
                 sqlite_column_name, data_type, mysql_data_type, description)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                """
                
                cursor.execute(insert_column_meta, (
                    table_meta_id, col_meta['index'], col_meta['original_name'],
                    col_meta['mysql_name'], col_meta['sqlite_name'],
                    col_meta['data_type'], col_meta['mysql_type'],
                    f"{col_meta['original_name']}åˆ—æ•°æ®"
                ))
            
            self.mysql_conn.commit()
            cursor.close()
            return True
            
        except Error as e:
            logger.error(f"âŒ è®°å½•è¡¨å…ƒæ•°æ®å¤±è´¥: {e}")
            return False
    
    def import_data_source(self, data_source: str, limit: Optional[int] = None) -> bool:
        """å¯¼å…¥æŒ‡å®šæ•°æ®æº"""
        logger.info(f"ğŸš€ å¼€å§‹å¯¼å…¥ {data_source} æ•°æ®é›†...")

        # æ–‡ä»¶è·¯å¾„
        sqlite_db = self.wikisql_path / f"{data_source}.db"

        if not sqlite_db.exists():
            logger.error(f"SQLiteæ•°æ®åº“ä¸å­˜åœ¨: {sqlite_db}")
            return False

        # åŠ è½½è¯­ä¹‰ä¿¡æ¯
        semantic_info = self.load_semantic_info(data_source)

        # è·å–SQLiteè¡¨åˆ—è¡¨
        sqlite_conn = sqlite3.connect(sqlite_db)
        cursor = sqlite_conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
        sqlite_tables = [row[0] for row in cursor.fetchall()]
        sqlite_conn.close()

        logger.info(f"å‘ç° {len(sqlite_tables)} ä¸ªSQLiteè¡¨")

        # é™åˆ¶å¤„ç†æ•°é‡
        if limit:
            sqlite_tables = sqlite_tables[:limit]
            logger.info(f"é™åˆ¶å¤„ç† {limit} ä¸ªè¡¨")

        success_count = 0
        error_count = 0

        # å¤„ç†æ¯ä¸ªè¡¨
        for table_name in tqdm(sqlite_tables, desc=f"å¯¼å…¥{data_source}è¡¨"):
            try:
                # ä»è¡¨åæå–table_id
                # table_1_10015132_11 -> 1-10015132-11
                parts = table_name.split('_')
                if len(parts) >= 4:
                    table_id = f"{parts[1]}-{parts[2]}-{parts[3]}"
                else:
                    logger.warning(f"æ— æ³•è§£æè¡¨ID: {table_name}")
                    continue

                # åˆ›å»ºMySQLè¡¨
                result = self.create_mysql_table_from_sqlite(
                    table_id, table_name, sqlite_db, semantic_info
                )

                if not result:
                    error_count += 1
                    continue

                mysql_table_name, columns_meta = result

                # å¯¼å…¥æ•°æ®
                if self.import_table_data(sqlite_db, table_name, mysql_table_name, columns_meta):
                    # è®°å½•å…ƒæ•°æ®
                    self.record_table_metadata(table_id, table_name, mysql_table_name,
                                             semantic_info.get(table_id, {}),
                                             columns_meta, data_source)
                    success_count += 1
                else:
                    error_count += 1

            except Exception as e:
                logger.error(f"å¤„ç†è¡¨ {table_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                error_count += 1
                continue

        logger.info(f"ğŸ‰ {data_source} æ•°æ®é›†å¯¼å…¥å®Œæˆï¼æˆåŠŸ: {success_count}, å¤±è´¥: {error_count}")
        return success_count > 0

    def get_import_statistics(self) -> Dict:
        """è·å–å¯¼å…¥ç»Ÿè®¡ä¿¡æ¯"""
        if not self.mysql_conn:
            return {}

        try:
            cursor = self.mysql_conn.cursor(dictionary=True)

            stats = {}

            # è¡¨ç»Ÿè®¡
            cursor.execute("""
                SELECT data_source, COUNT(*) as table_count
                FROM wikisql_tables_meta
                GROUP BY data_source
            """)
            stats['tables_by_source'] = {row['data_source']: row['table_count']
                                       for row in cursor.fetchall()}

            # æ€»ä½“ç»Ÿè®¡
            cursor.execute("SELECT COUNT(*) as total_tables FROM wikisql_tables_meta")
            stats['total_tables'] = cursor.fetchone()['total_tables']

            # æ•°æ®åº“å¤§å°
            cursor.execute("""
                SELECT
                    ROUND(SUM(data_length + index_length) / 1024 / 1024, 2) AS size_mb
                FROM information_schema.tables
                WHERE table_schema = DATABASE()
                AND table_name LIKE 'wikisql_%'
            """)
            result = cursor.fetchone()
            stats['database_size_mb'] = result['size_mb'] if result['size_mb'] else 0

            cursor.close()
            return stats

        except Exception as e:
            logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}

    def close_connection(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.mysql_conn:
            self.mysql_conn.close()
            logger.info("ğŸ”Œ æ•°æ®åº“è¿æ¥å·²å…³é—­")


def main():
    """ä¸»å‡½æ•°"""
    # MySQLé…ç½® (æ ¹æ®å¤–éƒ¨æœåŠ¡è„šæœ¬é…ç½®)
    mysql_config = {
        'host': 'localhost',
        'port': 3306,
        'database': 'nl2sql',
        'user': 'nl2sql_user',
        'password': 'nl2sql_pass',
        'charset': 'utf8mb4',
        'autocommit': False
    }

    # WikiSQLæ•°æ®è·¯å¾„ - ä½¿ç”¨ç›¸å¯¹è·¯å¾„
    import os
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    wikisql_data_path = os.path.join(project_root, "spring-ai-alibaba/spring-ai-alibaba-nl2sql/doc/pager/WikiSQL/data 2")

    # åˆ›å»ºå¯¼å…¥å™¨
    importer = WikiSQLMySQLImporter(mysql_config, wikisql_data_path)

    try:
        # 1. è¿æ¥MySQL
        if not importer.connect_mysql():
            logger.error("âŒ æ— æ³•è¿æ¥MySQLï¼Œè¯·æ£€æŸ¥æœåŠ¡çŠ¶æ€")
            return 1

        # 2. åˆ›å»ºå…ƒæ•°æ®è¡¨
        if not importer.create_metadata_tables():
            logger.error("âŒ åˆ›å»ºå…ƒæ•°æ®è¡¨å¤±è´¥")
            return 1

        # 3. å¯¼å…¥å¼€å‘é›†æ•°æ® (å®Œæ•´å¯¼å…¥)
        logger.info("ğŸš€ å¼€å§‹å¯¼å…¥å¼€å‘é›†æ•°æ®...")
        if importer.import_data_source('dev', limit=None):  # å®Œæ•´å¯¼å…¥æ‰€æœ‰è¡¨
            logger.info("âœ… å¼€å‘é›†æ•°æ®å¯¼å…¥æˆåŠŸ")

            # 4. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = importer.get_import_statistics()
            logger.info("ğŸ“Š å¯¼å…¥ç»Ÿè®¡ä¿¡æ¯:")
            for key, value in stats.items():
                logger.info(f"   {key}: {value}")

        else:
            logger.error("âŒ å¼€å‘é›†æ•°æ®å¯¼å…¥å¤±è´¥")
            return 1

        logger.info("ğŸ‰ WikiSQLæ•°æ®å¯¼å…¥å®Œæˆï¼")
        return 0

    except Exception as e:
        logger.error(f"âŒ å¯¼å…¥è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}")
        return 1
    finally:
        importer.close_connection()


if __name__ == "__main__":
    sys.exit(main())
