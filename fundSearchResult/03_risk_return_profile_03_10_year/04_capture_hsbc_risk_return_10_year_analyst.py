import asyncio
from pathlib import Path
import json
import csv
import random
import shutil
from datetime import datetime
from typing import List, Dict, Any, Tuple

from playwright.async_api import async_playwright, Request, Response

# Generate timestamp for this run
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# Output paths relative to script location (03_risk_return_profile_03_10_year directory)
SCRIPT_DIR = Path(__file__).resolve().parent
BASE_DIR = SCRIPT_DIR.parents[1]  # é¡¹ç›®æ ¹ç›®å½•
DATA_DIR = BASE_DIR / f"data_{timestamp}"
DATA_DIR.mkdir(parents=True, exist_ok=True)

# è¾“å‡ºç›®å½•ä¸ºè„šæœ¬æ‰€åœ¨ç›®å½• (03_risk_return_profile_03_10_year)
FUND_SEARCH_OUTPUT_DIR = SCRIPT_DIR
FUND_SEARCH_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# å­ç›®å½•ç»“æ„ - åœ¨03_risk_return_profile_03_10_yearä¸‹åˆ›å»º
LOG_DIR = FUND_SEARCH_OUTPUT_DIR / "log"
RESULT_DIR = FUND_SEARCH_OUTPUT_DIR / "result"
MAPPING_DIR = FUND_SEARCH_OUTPUT_DIR / "mapping"
LOG_DIR.mkdir(parents=True, exist_ok=True)
RESULT_DIR.mkdir(parents=True, exist_ok=True)
MAPPING_DIR.mkdir(parents=True, exist_ok=True)

def cleanup_previous_results():
    """æ¸…ç†ä¹‹å‰çš„ç»“æœæ–‡ä»¶ï¼ŒåŒ…æ‹¬æ—¥å¿—"""
    print("ğŸ§¹ å¼€å§‹æ¸…ç†ä¹‹å‰çš„ç»“æœæ–‡ä»¶...")

    cleaned_files = 0

    # æ¸…ç† result ç›®å½•
    if RESULT_DIR.exists():
        result_files = list(RESULT_DIR.glob('*'))
        for file_path in result_files:
            if file_path.is_file():
                file_path.unlink()
                cleaned_files += 1
        print(f"âœ… å·²æ¸…ç† result ç›®å½•ä¸­çš„ {len(result_files)} ä¸ªæ–‡ä»¶")

    # æ¸…ç† mapping ç›®å½•
    if MAPPING_DIR.exists():
        mapping_files = list(MAPPING_DIR.glob('*'))
        for file_path in mapping_files:
            if file_path.is_file():
                file_path.unlink()
                cleaned_files += 1
        print(f"âœ… å·²æ¸…ç† mapping ç›®å½•ä¸­çš„ {len(mapping_files)} ä¸ªæ–‡ä»¶")

    # æ¸…ç† log ç›®å½•
    if LOG_DIR.exists():
        log_files = list(LOG_DIR.glob('*'))
        for file_path in log_files:
            if file_path.is_file():
                file_path.unlink()
                cleaned_files += 1
        print(f"âœ… å·²æ¸…ç† log ç›®å½•ä¸­çš„ {len(log_files)} ä¸ªæ–‡ä»¶")

    # æ¸…ç†æ ¹ç›®å½•ä¸‹çš„æ–‡ä»¶
    root_files_to_clean = [
        "summary.json"
    ]

    # æ¸…ç†æ ¹ç›®å½•ä¸‹çš„ final_* æ–‡ä»¶
    final_files = list(FUND_SEARCH_OUTPUT_DIR.glob('final_*'))
    for file_path in final_files:
        if file_path.is_file():
            file_path.unlink()
            cleaned_files += 1

    # æ¸…ç†å…¶ä»–æ ¹ç›®å½•æ–‡ä»¶
    for filename in root_files_to_clean:
        file_path = FUND_SEARCH_OUTPUT_DIR / filename
        if file_path.exists():
            file_path.unlink()
            cleaned_files += 1

    if final_files or any((FUND_SEARCH_OUTPUT_DIR / f).exists() for f in root_files_to_clean):
        print(f"âœ… å·²æ¸…ç†æ ¹ç›®å½•ä¸­çš„ {len(final_files) + sum(1 for f in root_files_to_clean if (FUND_SEARCH_OUTPUT_DIR / f).exists())} ä¸ªæ–‡ä»¶")

    print(f"ğŸ¯ æ¸…ç†å®Œæˆï¼å…±æ¸…ç† {cleaned_files} ä¸ªæ–‡ä»¶\n")

# åŸºé‡‘è¯¦æƒ…é¡µé¢URLæ¨¡æ¿
FUND_DETAIL_URL_TEMPLATE = "https://investments3.personal-banking.hsbc.com.hk/public/utb/en-gb/fundDetail/{product_code}"

# åŸºé‡‘ç­›é€‰é¡µé¢URL
FUND_SCREENER_URL = "https://investments3.personal-banking.hsbc.com.hk/public/utb/en-gb/fundScreener"

# é»˜è®¤æµ‹è¯•çš„äº§å“ä»£ç 
DEFAULT_PRODUCT_CODE = "U43051"

# ç½‘ç»œè¯·æ±‚å­˜å‚¨
network_requests: List[Dict[str, Any]] = []
# å¿…é¡»æ•è·çš„8ä¸ªæ ¸å¿ƒAPIï¼ˆå®Œæ•´æ€§æ ¡éªŒç”¨ï¼‰
REQUIRED_APIS = {
    "amh_ut_product",
    "wmds_quoteDetail",
    "wmds_fundQuoteSummary",
    "wmds_holdingAllocation",
    "wmds_topTenHoldings",
    "wmds_otherFundClasses",
    "wmds_fundSearchCriteria",
    "wmds_advanceChart",
}

# fundSearchResult API ç›¸å…³é…ç½®
FUND_SEARCH_RESULT_API = "wmds_fundSearchResult"
REQUIRED_FUND_SEARCH_APIS = {FUND_SEARCH_RESULT_API}

network_responses: List[Dict[str, Any]] = []

def read_product_codes_csv(csv_file_path: str) -> List[Tuple[str, int]]:
    """è¯»å–äº§å“ä»£ç CSVæ–‡ä»¶"""
    product_codes = []
    csv_path = Path(csv_file_path)

    if not csv_path.exists():
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")
        return product_codes

    try:
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                product_code = row.get('Product_Code', '').strip()
                index = int(row.get('Index', 0))
                if product_code:
                    product_codes.append((product_code, index))

        print(f"âœ… æˆåŠŸè¯»å–CSVæ–‡ä»¶: {len(product_codes)}ä¸ªäº§å“ä»£ç ")
        return product_codes

    except Exception as e:
        print(f"âŒ è¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return product_codes

def should_capture_request(request: Request) -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ•è·è¯¥è¯·æ±‚"""
    url = request.url.lower()
    resource_type = request.resource_type

    # è¿‡æ»¤æ‰é™æ€èµ„æº
    static_extensions = ['.js', '.css', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico',
                        '.woff', '.woff2', '.ttf', '.otf', '.eot', '.webp', '.bmp', '.html']

    # è¿‡æ»¤æ‰é™æ€èµ„æºç±»å‹
    static_resource_types = ['stylesheet', 'script', 'image', 'font', 'media', 'document']

    # è¿‡æ»¤æ‰åŒ…å«é™æ€èµ„æºå…³é”®è¯çš„URL
    static_keywords = ['/css/', '/js/', '/fonts/', '/images/', '/img/', '/static/',
                      'stylesheet', 'javascript', '.min.js', '.min.css', 'bundle.js',
                      'bundle.css', 'jquery', 'bootstrap', 'fontawesome']

    # è¿‡æ»¤æ‰ç‰¹å®šçš„APIç«¯ç‚¹å’Œç¬¬ä¸‰æ–¹æœåŠ¡
    excluded_apis = [
        '/amh/ut/sdktoken',
        'funddetail/',  # ä¸»é¡µé¢è¯·æ±‚
        'logx.optimizely.com',  # Optimizelyæ—¥å¿—
        'lpcdn-vip.lpsnmedia.net',  # LivePersonå®¢æœ
        'match.adsrvr.org',  # å¹¿å‘ŠåŒ¹é…
        'quotespeed.morningstar.com',  # æ™¨æ˜ŸæœåŠ¡
        'sp.analytics.yahoo.com',  # Yahooåˆ†æ
        'td.doubleclick.net',  # Google DoubleClick
        'www.google.com/ccm',  # Google CCM
        'www.googletagmanager.com',  # Google Tag Manager
        'www.facebook.com/tr',  # Facebookåƒç´ 
        'visitor-service-ap-northeast-1.tealiumiq.com',  # Tealiumè®¿å®¢æœåŠ¡
        'googleads.g.doubleclick.net',  # Googleå¹¿å‘Š
        'google.com/ccm/form-data',  # Googleè¡¨å•æ•°æ®
        'bat.bing.com',  # Bingå¹¿å‘Š
        'cm.g.doubleclick.net',  # DoubleClick CookieåŒ¹é…
        'connect.facebook.net',  # Facebookè¿æ¥
        'accdn-vip.lpsnmedia.net',  # LivePerson CDN
        'a19069622224.cdn.optimizely.com'  # Optimizely CDN
    ]

    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
    for ext in static_extensions:
        if ext in url:
            return False

    # æ£€æŸ¥èµ„æºç±»å‹
    if resource_type in static_resource_types:
        return False

    # æ£€æŸ¥URLå…³é”®è¯
    for keyword in static_keywords:
        if keyword in url:
            return False

    # æ£€æŸ¥æ’é™¤çš„APIç«¯ç‚¹å’ŒåŸŸå
    for api in excluded_apis:
        if api in url:
            return False

    # åªä¿ç•™HSBCæ ¸å¿ƒAPIå’Œæ™¨æ˜Ÿæ•°æ®API
    hsbc_api_patterns = [
        'investments3.personal-banking.hsbc.com.hk/shp/wealth-mobile-',
        'quotespeed.morningstar.com/ra/'
    ]

    # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬éœ€è¦çš„API
    for pattern in hsbc_api_patterns:
        if pattern in url:
            return True

    # é»˜è®¤ä¸æ•è·å…¶ä»–è¯·æ±‚
    return False

async def capture_request(request: Request):
    """æ•è·HTTPè¯·æ±‚ï¼ˆè¿‡æ»¤é™æ€èµ„æºï¼‰"""
    try:
        # è¿‡æ»¤é™æ€èµ„æº
        if not should_capture_request(request):
            return

        request_data = {
            "timestamp": datetime.now().isoformat(),
            "method": request.method,
            "url": request.url,
            "headers": dict(request.headers),
            "resource_type": request.resource_type,
            "post_data": request.post_data if request.post_data else None,
            "is_navigation_request": request.is_navigation_request()
        }
        network_requests.append(request_data)
        print(f"ğŸ“¤ è¯·æ±‚: {request.method} {request.url}")
    except Exception as e:
        print(f"âš ï¸  æ•è·è¯·æ±‚æ—¶å‡ºé”™: {e}")

def should_capture_response(response: Response) -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ•è·è¯¥å“åº”ï¼ˆä¸è¯·æ±‚è¿‡æ»¤é€»è¾‘ä¸€è‡´ï¼‰"""
    url = response.url.lower()

    # è¿‡æ»¤æ‰é™æ€èµ„æº
    static_extensions = ['.js', '.css', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico',
                        '.woff', '.woff2', '.ttf', '.otf', '.eot', '.webp', '.bmp', '.html']

    # è¿‡æ»¤æ‰åŒ…å«é™æ€èµ„æºå…³é”®è¯çš„URL
    static_keywords = ['/css/', '/js/', '/fonts/', '/images/', '/img/', '/static/',
                      'stylesheet', 'javascript', '.min.js', '.min.css', 'bundle.js',
                      'bundle.css', 'jquery', 'bootstrap', 'fontawesome']

    # è¿‡æ»¤æ‰ç‰¹å®šçš„APIç«¯ç‚¹å’Œç¬¬ä¸‰æ–¹æœåŠ¡
    excluded_apis = [
        '/amh/ut/sdktoken',
        'funddetail/',  # ä¸»é¡µé¢è¯·æ±‚
        'logx.optimizely.com',  # Optimizelyæ—¥å¿—
        'lpcdn-vip.lpsnmedia.net',  # LivePersonå®¢æœ
        'match.adsrvr.org',  # å¹¿å‘ŠåŒ¹é…
        'quotespeed.morningstar.com',  # æ™¨æ˜ŸæœåŠ¡
        'sp.analytics.yahoo.com',  # Yahooåˆ†æ
        'td.doubleclick.net',  # Google DoubleClick
        'www.google.com/ccm',  # Google CCM
        'www.googletagmanager.com',  # Google Tag Manager
        'www.facebook.com/tr',  # Facebookåƒç´ 
        'visitor-service-ap-northeast-1.tealiumiq.com',  # Tealiumè®¿å®¢æœåŠ¡
        'googleads.g.doubleclick.net',  # Googleå¹¿å‘Š
        'google.com/ccm/form-data',  # Googleè¡¨å•æ•°æ®
        'bat.bing.com',  # Bingå¹¿å‘Š
        'cm.g.doubleclick.net',  # DoubleClick CookieåŒ¹é…
        'connect.facebook.net',  # Facebookè¿æ¥
        'accdn-vip.lpsnmedia.net',  # LivePerson CDN
        'a19069622224.cdn.optimizely.com'  # Optimizely CDN
    ]

    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
    for ext in static_extensions:
        if ext in url:
            return False

    # æ£€æŸ¥URLå…³é”®è¯
    for keyword in static_keywords:
        if keyword in url:
            return False

    # æ£€æŸ¥æ’é™¤çš„APIç«¯ç‚¹å’ŒåŸŸå
    for api in excluded_apis:
        if api in url:
            return False

    # åªä¿ç•™HSBCæ ¸å¿ƒAPIå’Œæ™¨æ˜Ÿæ•°æ®API
    hsbc_api_patterns = [
        'investments3.personal-banking.hsbc.com.hk/shp/wealth-mobile-',
        'quotespeed.morningstar.com/ra/'
    ]

    # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬éœ€è¦çš„API
    for pattern in hsbc_api_patterns:
        if pattern in url:
            return True

    # é»˜è®¤ä¸æ•è·å…¶ä»–å“åº”
    return False

async def capture_response(response: Response):
    """æ•è·HTTPå“åº”ï¼ˆè¿‡æ»¤é™æ€èµ„æºï¼‰"""
    try:
        # è¿‡æ»¤é™æ€èµ„æº
        if not should_capture_response(response):
            return

        # è·å–å“åº”ä½“ï¼ˆä»…å¯¹ç‰¹å®šç±»å‹ï¼‰
        response_body = None
        content_type = response.headers.get("content-type", "")

        if any(t in content_type.lower() for t in ["json", "text", "xml", "html"]):
            try:
                response_body = await response.text()
            except:
                response_body = None

        response_data = {
            "timestamp": datetime.now().isoformat(),
            "url": response.url,
            "status": response.status,
            "status_text": response.status_text,
            "headers": dict(response.headers),
            "content_type": content_type,
            "response_body": response_body,
            "ok": response.ok,
            "from_service_worker": response.from_service_worker
        }
        network_responses.append(response_data)
        print(f"ğŸ“¥ å“åº”: {response.status} {response.url}")
    except Exception as e:
        print(f"âš ï¸  æ•è·å“åº”æ—¶å‡ºé”™: {e}")

def extract_api_name(url: str) -> str:
    """ä»URLä¸­æå–APIåç§°"""
    # æŸ¥æ‰¾HSBC APIè·¯å¾„
    if '/shp/wealth-mobile-' in url:
        # æå–APIè·¯å¾„çš„æœ€åéƒ¨åˆ†
        parts = url.split('/')
        for i, part in enumerate(parts):
            if part.startswith('v0') and i + 1 < len(parts):
                # è·å–v0åé¢çš„è·¯å¾„éƒ¨åˆ†ï¼Œç”¨ä¸‹åˆ’çº¿è¿æ¥
                api_parts = parts[i+1:]
                # ç§»é™¤æŸ¥è¯¢å‚æ•°
                if '?' in api_parts[-1]:
                    api_parts[-1] = api_parts[-1].split('?')[0]
                return '_'.join(api_parts)

    # å¯¹äºå…¶ä»–URLï¼Œå°è¯•æå–æœ‰æ„ä¹‰çš„éƒ¨åˆ†
    if 'morningstar.com' in url:
        if '/ra/' in url:
            path_part = url.split('/ra/')[-1].split('?')[0]
            return f"morningstar_{path_part}"

    # é»˜è®¤æƒ…å†µï¼Œä½¿ç”¨åŸŸåå’Œè·¯å¾„
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc.split('.')[0]  # å–åŸŸåç¬¬ä¸€éƒ¨åˆ†
        path = parsed.path.strip('/').replace('/', '_')
        if path:
            return f"{domain}_{path}"
        else:
            return domain
    except:
        return "unknown_api"

async def save_network_data(product_code: str, item_number: int, task_requests: List[Dict[str, Any]], task_responses: List[Dict[str, Any]]):
    """ä¿å­˜ç½‘ç»œæ•°æ®åˆ°ä¸´æ—¶ç›®å½•å¹¶åŸå­è½ç›˜åˆ°æœ€ç»ˆç›®å½•ï¼Œç¡®ä¿åªåœ¨å®Œæˆæ—¶ç”Ÿæˆè¾“å‡º"""

    # ç›®å½•: ä½¿ç”¨ç°æœ‰çš„ .NNN_Code.working ç›®å½•ï¼ˆç”±è°ƒåº¦å™¨/workeråˆ›å»ºï¼‰
    working_dir = DATA_DIR / f".{item_number:03d}_{product_code}.working"
    working_dir.mkdir(parents=True, exist_ok=True)
    # æœ€ç»ˆç›®å½•ç”±å¤–å±‚ complete_work æ§åˆ¶åˆ‡æ¢ä¸º .done


    # æŒ‰APIåˆ†ç»„ä¿å­˜è¯·æ±‚å’Œå“åº”
    api_groups: Dict[str, Dict[str, List[Dict[str, Any]]]] = {}

    for req in task_requests:
        api_name = extract_api_name(req["url"])
        api_groups.setdefault(api_name, {"requests": [], "responses": []})
        api_groups[api_name]["requests"].append(req)

    for resp in task_responses:
        api_name = extract_api_name(resp["url"])
        api_groups.setdefault(api_name, {"requests": [], "responses": []})
        api_groups[api_name]["responses"].append(resp)

    saved_files = []

    # ä¸ºæ¯ä¸ªAPIä¿å­˜å•ç‹¬çš„è¯·æ±‚å’Œå“åº”æ–‡ä»¶ï¼ˆå†™åˆ°å·¥ä½œç›®å½•ï¼‰
    for api_name, data in api_groups.items():
        if data["requests"]:
            request_file = working_dir / f"{api_name}.request.json"
            with open(request_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "api_name": api_name,
                    "product_code": product_code,
                    "timestamp": timestamp,
                    "total_requests": len(data["requests"]),
                    "requests": data["requests"]
                }, f, indent=2, ensure_ascii=False)
            saved_files.append(request_file)

        if data["responses"]:
            response_file = working_dir / f"{api_name}.response.json"
            with open(response_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "api_name": api_name,
                    "product_code": product_code,
                    "timestamp": timestamp,
                    "total_responses": len(data["responses"]),
                    "responses": data["responses"]
                }, f, indent=2, ensure_ascii=False)
            saved_files.append(response_file)

    # ä¿å­˜æ±‡æ€»ä¿¡æ¯ï¼ˆå†™åˆ°å·¥ä½œç›®å½•ï¼‰
    summary_file = working_dir / "summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump({
            "product_code": product_code,
            "timestamp": timestamp,
            "session_dir": str(working_dir),
            "summary": {
                "total_requests": len(task_requests),
                "total_responses": len(task_responses),
                "api_count": len(api_groups),
                "api_names": list(api_groups.keys()),
                "unique_domains": list(set([req["url"].split("/")[2] for req in task_requests if "url" in req])),
                "request_methods": list(set([req["method"] for req in task_requests if "method" in req])),
                "response_status_codes": list(set([resp["status"] for resp in task_responses if "status" in resp]))
            }
        }, f, indent=2, ensure_ascii=False)
    saved_files.append(summary_file)

    # ç§»åŠ¨æˆªå›¾æ–‡ä»¶åˆ°å·¥ä½œç›®å½•
    temp_screenshot_file = DATA_DIR / f"fund_detail_{product_code}_{timestamp}.png"
    if temp_screenshot_file.exists():
        final_screenshot_file = working_dir / f"fund_detail_{product_code}_{timestamp}.png"
        temp_screenshot_file.rename(final_screenshot_file)
        saved_files.append(final_screenshot_file)
        print(f"ğŸ“¸ æˆªå›¾å·²ç§»åŠ¨åˆ°å·¥ä½œç›®å½•: {final_screenshot_file}")

    # å®Œæ•´æ€§æ ¡éªŒï¼šè¦æ±‚8ä¸ªæ ¸å¿ƒAPIå…¨éƒ¨å­˜åœ¨ä¸”æ¯ä¸ªè‡³å°‘æœ‰1ä¸ªå“åº”
    found_api_names = set(api_groups.keys())
    missing = list(REQUIRED_APIS - found_api_names)
    has_all_required = len(missing) == 0 and all(len(api_groups.get(api, {}).get("responses", [])) > 0 for api in REQUIRED_APIS)

    if not has_all_required:
        # ä¸è½ç›˜æœ€ç»ˆç›®å½•ï¼Œä¿ç•™å·¥ä½œç›®å½•ç”¨äºè¯Šæ–­ï¼Œå¹¶è¿”å›å¤±è´¥æ ‡è®°
        print(f"âŒ å®Œæ•´æ€§æ ¡éªŒå¤±è´¥ï¼šç¼ºå°‘API {missing} æˆ–æŸäº›APIæ— å“åº”ã€‚ä¿ç•™å·¥ä½œç›®å½•: {working_dir}")
        final_effective = None
        return {
            "session_dir": final_effective,
            "saved_files": saved_files,
            "api_groups": api_groups,
            "complete": False,
            "missing": missing,
        }

    # åœ¨å¹¶å‘é˜Ÿåˆ—æ¨¡å¼ä¸‹ï¼Œä¸åœ¨æ­¤å¤„é‡å‘½åæœ€ç»ˆç›®å½•ï¼›ç”± complete_work ç»Ÿä¸€åˆ‡æ¢ä¸º .done
    final_effective = working_dir

    print(f"\nğŸ’¾ ç½‘ç»œæ•°æ®å·²å†™å…¥å·¥ä½œç›®å½•: {final_effective}")
    print(f"ğŸ“Š APIåˆ†ç»„æ•°é‡: {len(api_groups)}")
    for api_name in sorted(api_groups.keys()):
        req_count = len(api_groups[api_name]["requests"])
        resp_count = len(api_groups[api_name]["responses"])
        print(f"   ğŸ“ {api_name}: {req_count}ä¸ªè¯·æ±‚, {resp_count}ä¸ªå“åº”")

    return {
        "session_dir": final_effective,
        "saved_files": saved_files,
        "api_groups": api_groups,
        "complete": True
    }

async def capture_fund_screener_api(headless: bool = True, save_to_dir: Path = None) -> Dict[str, Any]:
    """æ•è·åŸºé‡‘ç­›é€‰é¡µé¢çš„ fundSearchResult API è¯·æ±‚å’Œå“åº”"""

    # ä¸ºåŸºé‡‘ç­›é€‰ä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„ç½‘ç»œè¯·æ±‚å­˜å‚¨
    screener_requests: List[Dict[str, Any]] = []
    screener_responses: List[Dict[str, Any]] = []

    # æ§åˆ¶æ˜¯å¦å¼€å§‹æ•è·çš„æ ‡å¿—
    capture_enabled = False

    # è°ƒè¯•æ—¥å¿—å­˜å‚¨
    debug_logs: List[Dict[str, Any]] = []

    def log_debug(step: str, action: str, details: Dict[str, Any]):
        """è®°å½•è°ƒè¯•ä¿¡æ¯"""
        debug_entry = {
            "timestamp": datetime.now().isoformat(),
            "step": step,
            "action": action,
            "details": details
        }
        debug_logs.append(debug_entry)
        print(f"ğŸ” [è°ƒè¯•] {step} - {action}: {details}")

    def should_capture_fund_search_request(request: Request) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ•è· fundSearchResult API è¯·æ±‚"""
        if not capture_enabled:
            return False

        url = request.url.lower()

        # æ‰©å¤§æ•è·èŒƒå›´ï¼ŒåŒ…å«å¤šç§å¯èƒ½çš„APIç«¯ç‚¹
        target_patterns = [
            'fundsearchresult',  # åŸºæœ¬æ¨¡å¼
            'fund-search-result',  # å¯èƒ½çš„å˜ä½“
            'fund_search_result',  # ä¸‹åˆ’çº¿å˜ä½“
            'search/fund',  # å¯èƒ½çš„è·¯å¾„å˜ä½“
            'fund/search',  # å¦ä¸€ç§è·¯å¾„å˜ä½“
            'wmds/fund',  # åŒ…å«fundçš„wmds API
            'investment/fund',  # æŠ•èµ„ç›¸å…³API
        ]

        # æ£€æŸ¥URLæ˜¯å¦åŒ…å«ä»»ä½•ç›®æ ‡æ¨¡å¼
        for pattern in target_patterns:
            if pattern in url:
                print(f"ğŸ¯ æ£€æµ‹åˆ°å¯èƒ½çš„åŸºé‡‘API: {pattern} in {url}")
                return True

        return False

    async def capture_request(request: Request):
        """æ•è·HTTPè¯·æ±‚ï¼ˆåŸºé‡‘ç­›é€‰ä¸“ç”¨ï¼‰"""
        nonlocal capture_enabled
        try:
            # è®°å½•æ‰€æœ‰è¯·æ±‚ç”¨äºè°ƒè¯•
            if capture_enabled:
                print(f"ğŸ” [è°ƒè¯•] æ£€æŸ¥è¯·æ±‚: {request.method} {request.url}")

            # åªåœ¨å¯ç”¨æ•è·åæ‰æ•è·åŸºé‡‘æœç´¢ç›¸å…³çš„API
            if not should_capture_fund_search_request(request):
                return

            request_data = {
                "timestamp": datetime.now().isoformat(),
                "method": request.method,
                "url": request.url,
                "headers": dict(request.headers),
                "resource_type": request.resource_type,
                "post_data": request.post_data if request.post_data else None,
                "is_navigation_request": request.is_navigation_request()
            }
            screener_requests.append(request_data)
            print(f"ğŸ“¤ [ç­›é€‰å™¨] è¯·æ±‚: {request.method} {request.url}")
        except Exception as e:
            print(f"âš ï¸  [ç­›é€‰å™¨] æ•è·è¯·æ±‚æ—¶å‡ºé”™: {e}")

    async def capture_response(response: Response):
        """æ•è·HTTPå“åº”ï¼ˆåŸºé‡‘ç­›é€‰ä¸“ç”¨ï¼‰"""
        nonlocal capture_enabled
        try:
            # åªåœ¨å¯ç”¨æ•è·åæ‰æ•è·åŸºé‡‘æœç´¢ç›¸å…³çš„API
            if not should_capture_fund_search_request(response.request):
                return

            # è·å–å“åº”ä½“
            response_body = None
            content_type = response.headers.get("content-type", "")

            if any(t in content_type.lower() for t in ["json", "text", "xml", "html"]):
                try:
                    response_body = await response.text()
                except:
                    response_body = None

            response_data = {
                "timestamp": datetime.now().isoformat(),
                "url": response.url,
                "status": response.status,
                "status_text": response.status_text,
                "headers": dict(response.headers),
                "content_type": content_type,
                "response_body": response_body,
                "ok": response.ok,
                "from_service_worker": response.from_service_worker
            }
            screener_responses.append(response_data)
            print(f"ğŸ“¥ [ç­›é€‰å™¨] å“åº”: {response.status} {response.url}")
        except Exception as e:
            print(f"âš ï¸  [ç­›é€‰å™¨] æ•è·å“åº”æ—¶å‡ºé”™: {e}")

    print("="*60)
    print(f"ğŸš€ å¼€å§‹æ•è·åŸºé‡‘ç­›é€‰é¡µé¢ fundSearchResult API")
    print(f"ğŸŒ ç›®æ ‡URL: {FUND_SCREENER_URL}")
    print("="*60)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=headless)
        context = await browser.new_context()
        page = await context.new_page()

        # è®¾ç½®ç½‘ç»œç›‘å¬å™¨
        page.on("request", capture_request)
        page.on("response", capture_response)

        # å®šä¹‰å¯ç”¨æ•è·çš„å‡½æ•°
        def enable_capture():
            nonlocal capture_enabled
            capture_enabled = True

        def disable_capture():
            nonlocal capture_enabled
            capture_enabled = False

        try:
            # æå‰å¯ç”¨ç½‘ç»œæ•è·ï¼Œç¡®ä¿ä¸é”™è¿‡ä»»ä½•APIè¯·æ±‚
            print("ğŸ”„ æå‰å¯ç”¨ç½‘ç»œæ•è·...")
            enable_capture()

            print(f"\n=== æ­¥éª¤ 1: è®¿é—®åŸºé‡‘ç­›é€‰é¡µé¢ ===")
            await page.goto(FUND_SCREENER_URL, wait_until="domcontentloaded", timeout=60000)
            print(f"âœ… é¡µé¢å·²åŠ è½½: {page.url}")

            # ç­‰å¾…é¡µé¢åŠ è½½
            print("â³ ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ...")
            try:
                await page.wait_for_load_state("networkidle", timeout=60000)
                print("âœ… é¡µé¢ç½‘ç»œç©ºé—²çŠ¶æ€è¾¾åˆ°")
            except Exception as e:
                print(f"âš ï¸  ç½‘ç»œç©ºé—²ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­æ‰§è¡Œ: {e}")

            # ç­‰å¾…å…³é”®å…ƒç´ å‡ºç° - ä½¿ç”¨å·²éªŒè¯çš„é€‰æ‹©å™¨
            print("â³ ç­‰å¾…é£é™©åå¥½ä¸‹æ‹‰æ¡†å‡ºç°...")
            try:
                await page.wait_for_selector('#dropdown-riskLevel', timeout=30000)
                print("âœ… é£é™©åå¥½ä¸‹æ‹‰æ¡†å·²æ‰¾åˆ°")
                log_debug("æ­¥éª¤2", "æ‰¾åˆ°ä¸‹æ‹‰æ¡†", {"selector": "#dropdown-riskLevel", "element_type": "Aæ ‡ç­¾"})

            except Exception as e:
                print(f"âŒ æœªæ‰¾åˆ°é£é™©åå¥½ä¸‹æ‹‰æ¡†: {e}")
                log_debug("æ­¥éª¤2", "é”™è¯¯", {"error": str(e), "selector": "#dropdown-riskLevel"})
                return {
                    "requests": screener_requests,
                    "responses": screener_responses,
                    "api_captured": False
                }

            print(f"\n=== æ­¥éª¤ 2: è®¾ç½®é£é™©ç­‰çº§ä¸º Speculative - 5 ===")
            # ç‚¹å‡»é£é™©åå¥½ä¸‹æ‹‰æ¡† - ä½¿ç”¨å·²éªŒè¯çš„é€‰æ‹©å™¨
            await page.click('#dropdown-riskLevel')
            print("âœ… å·²ç‚¹å‡»é£é™©åå¥½ä¸‹æ‹‰æ¡†")
            log_debug("æ­¥éª¤2", "ç‚¹å‡»ä¸‹æ‹‰æ¡†", {"selector": "#dropdown-riskLevel", "success": True})
            await page.wait_for_timeout(1000)  # å‡å°‘ç­‰å¾…æ—¶é—´

            # é€‰æ‹© Speculative - 5 é€‰é¡¹ - ä½¿ç”¨å·²éªŒè¯æœ‰æ•ˆçš„é€‰æ‹©å™¨
            try:
                # ç­‰å¾…é€‰é¡¹å‡ºç° - ä½¿ç”¨å·²éªŒè¯çš„é€‰æ‹©å™¨ï¼Œå‡å°‘è¶…æ—¶æ—¶é—´
                await page.wait_for_selector('li:has-text("Speculative - 5")', timeout=5000)

                # ç›´æ¥ç‚¹å‡»å·²éªŒè¯æœ‰æ•ˆçš„é€‰æ‹©å™¨
                await page.click('li:has-text("Speculative - 5")')
                print("âœ… å·²é€‰æ‹© Speculative - 5")
                log_debug("æ­¥éª¤2", "æˆåŠŸé€‰æ‹©é€‰é¡¹", {
                    "selector": 'li:has-text("Speculative - 5")',
                    "method": "direct_click",
                    "success": True
                })
                speculative_option_found = True

            except Exception as e:
                print(f"âŒ é€‰æ‹© Speculative - 5 æ—¶å‡ºé”™: {e}")
                log_debug("æ­¥éª¤2", "é€‰æ‹©é€‰é¡¹å¤±è´¥", {
                    "selector": 'li:has-text("Speculative - 5")',
                    "error": str(e)
                })
                speculative_option_found = False

            if not speculative_option_found:
                print("âŒ æœªèƒ½é€‰æ‹© Speculative - 5 é€‰é¡¹")
                return {
                    "requests": screener_requests,
                    "responses": screener_responses,
                    "api_captured": False
                }

            # ç§»é™¤éªŒè¯æ­¥éª¤ä»¥æé«˜æ•ˆç‡
            await page.wait_for_timeout(500)  # æœ€å°ç­‰å¾…æ—¶é—´

            print(f"\n=== æ­¥éª¤ 3: æ‰§è¡Œæœç´¢ ===")

            # ç­‰å¾…æœç´¢æŒ‰é’® - ä½¿ç”¨å·²éªŒè¯æœ‰æ•ˆçš„é€‰æ‹©å™¨ï¼Œå‡å°‘è¶…æ—¶æ—¶é—´
            try:
                await page.wait_for_selector('a[data-testid="Search"]', timeout=5000)
                print("âœ… æ‰¾åˆ°æœç´¢æŒ‰é’®")
                log_debug("æ­¥éª¤3", "æ‰¾åˆ°æœç´¢æŒ‰é’®", {
                    "selector": 'a[data-testid="Search"]',
                    "element_type": "Aæ ‡ç­¾",
                    "success": True
                })
            except Exception as e:
                print(f"âŒ æœªæ‰¾åˆ°æœç´¢æŒ‰é’®: {e}")
                log_debug("æ­¥éª¤3", "æœç´¢æŒ‰é’®å¤±è´¥", {
                    "selector": 'a[data-testid="Search"]',
                    "error": str(e)
                })
                return {
                    "requests": screener_requests,
                    "responses": screener_responses,
                    "api_captured": False
                }

            # ç‚¹å‡»æœç´¢æŒ‰é’® - ä½¿ç”¨å·²éªŒè¯æœ‰æ•ˆçš„é€‰æ‹©å™¨
            try:
                await page.click('a[data-testid="Search"]')
                print("âœ… å·²ç‚¹å‡»æœç´¢æŒ‰é’®")
                log_debug("æ­¥éª¤3", "æˆåŠŸç‚¹å‡»æœç´¢æŒ‰é’®", {
                    "selector": 'a[data-testid="Search"]',
                    "method": "direct_click",
                    "success": True
                })
            except Exception as e:
                print(f"âŒ ç‚¹å‡»æœç´¢æŒ‰é’®å¤±è´¥: {e}")
                log_debug("æ­¥éª¤3", "ç‚¹å‡»æœç´¢æŒ‰é’®å¤±è´¥", {
                    "selector": 'a[data-testid="Search"]',
                    "error": str(e)
                })
                return {
                    "requests": screener_requests,
                    "responses": screener_responses,
                    "api_captured": False
                }

            await page.wait_for_timeout(3000)  # å‡å°‘ç­‰å¾…æ—¶é—´

            print(f"âœ… å½“å‰é¡µé¢URL: {page.url}")

            print(f"\n=== æ­¥éª¤ 4: ç‚¹å‡» Risk return profile æ ‡ç­¾ ===")
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆï¼Œç¡®ä¿æ ‡ç­¾é¡µå¯è§
            await page.wait_for_timeout(2000)

            try:
                # æŸ¥æ‰¾å¹¶ç‚¹å‡» "Risk return profile" æ ‡ç­¾
                risk_return_selector = 'button:has-text("Risk return profile"), a:has-text("Risk return profile"), [data-testid*="risk-return"], [data-testid*="Risk-return"]'

                # ç­‰å¾… Risk return profile æ ‡ç­¾å‡ºç°
                await page.wait_for_selector(risk_return_selector, timeout=10000)
                print("âœ… æ‰¾åˆ° Risk return profile æ ‡ç­¾")

                # ç‚¹å‡» Risk return profile æ ‡ç­¾
                await page.click(risk_return_selector)
                print("âœ… å·²ç‚¹å‡» Risk return profile æ ‡ç­¾")
                log_debug("æ­¥éª¤4", "æˆåŠŸç‚¹å‡»Risk return profileæ ‡ç­¾", {
                    "selector": risk_return_selector,
                    "success": True
                })

                # ç­‰å¾…æ ‡ç­¾é¡µåˆ‡æ¢å®Œæˆ
                await page.wait_for_timeout(2000)

                # éªŒè¯æ˜¯å¦æˆåŠŸåˆ‡æ¢åˆ° Risk return profile é¡µé¢
                current_url = page.url
                print(f"âœ… Risk return profile é¡µé¢URL: {current_url}")

                # ç¡®è®¤ç‚¹å‡»æˆåŠŸ - æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å« Risk return profile ç›¸å…³å†…å®¹
                risk_return_content = await page.query_selector('[data-testid*="risk-return"], .risk-return, #risk-return')
                if risk_return_content or "risk" in current_url.lower():
                    print("âœ… ç¡®è®¤æˆåŠŸåˆ‡æ¢åˆ° Risk return profile é¡µé¢")
                    log_debug("æ­¥éª¤4", "Risk return profileé¡µé¢åˆ‡æ¢æˆåŠŸ", {
                        "url": current_url,
                        "content_found": risk_return_content is not None,
                        "success": True
                    })
                else:
                    print("âš ï¸  æ— æ³•ç¡®è®¤æ˜¯å¦æˆåŠŸåˆ‡æ¢åˆ° Risk return profile é¡µé¢ï¼Œç»§ç»­æ‰§è¡Œ...")
                    log_debug("æ­¥éª¤4", "Risk return profileé¡µé¢åˆ‡æ¢çŠ¶æ€ä¸æ˜", {
                        "url": current_url,
                        "content_found": False,
                        "warning": True
                    })

                # æ­¥éª¤ 4.1: ç‚¹å‡» 10 year å­æ ‡ç­¾
                print(f"\n=== æ­¥éª¤ 4.1: ç‚¹å‡» 10 year å­æ ‡ç­¾ ===")
                try:
                    # æŸ¥æ‰¾å¹¶ç‚¹å‡» "10 year" å­æ ‡ç­¾
                    ten_year_selector = 'li:has-text("10 year"), button:has-text("10 year"), a:has-text("10 year"), [data-testid*="10-year"]'

                    # ç­‰å¾… 10 year å­æ ‡ç­¾å‡ºç°
                    await page.wait_for_selector(ten_year_selector, timeout=10000)
                    print("âœ… æ‰¾åˆ° 10 year å­æ ‡ç­¾")

                    # ç‚¹å‡» 10 year å­æ ‡ç­¾
                    await page.click(ten_year_selector)
                    print("âœ… å·²ç‚¹å‡» 10 year å­æ ‡ç­¾")
                    log_debug("æ­¥éª¤4.1", "æˆåŠŸç‚¹å‡»10 yearå­æ ‡ç­¾", {
                        "selector": ten_year_selector,
                        "success": True
                    })

                    # ç­‰å¾…å­æ ‡ç­¾åˆ‡æ¢å®Œæˆ
                    await page.wait_for_timeout(2000)

                except Exception as e:
                    print(f"âŒ ç‚¹å‡» 10 year å­æ ‡ç­¾å¤±è´¥: {e}")
                    log_debug("æ­¥éª¤4.1", "10 yearå­æ ‡ç­¾ç‚¹å‡»å¤±è´¥", {
                        "error": str(e),
                        "selector": ten_year_selector
                    })
                    print("âš ï¸  ç»§ç»­æ‰§è¡Œï¼Œä½†å¯èƒ½æ˜¾ç¤ºçš„æ˜¯é»˜è®¤æ—¶é—´æ®µæ•°æ®")

            except Exception as e:
                print(f"âŒ ç‚¹å‡» Risk return profile æ ‡ç­¾å¤±è´¥: {e}")
                log_debug("æ­¥éª¤4", "Risk return profileæ ‡ç­¾ç‚¹å‡»å¤±è´¥", {
                    "error": str(e),
                    "selector": risk_return_selector
                })
                print("âš ï¸  ç»§ç»­æ‰§è¡Œï¼Œä½†å¯èƒ½æ— æ³•è·å– Risk return profile æ•°æ®")

            print("ğŸ“‹ ä½¿ç”¨é»˜è®¤æ˜¾ç¤ºæ•°é‡ï¼Œç­‰å¾…APIå“åº”...")
            log_debug("æ­¥éª¤5", "ä½¿ç”¨é»˜è®¤æ˜¾ç¤ºæ•°é‡", {
                "action": "skip_display_adjustment",
                "reason": "ä½¿ç”¨é¡µé¢é»˜è®¤æ˜¾ç¤ºæ•°é‡",
                "success": True
            })

            print("â³ ç­‰å¾… fundSearchResult API å“åº”...")

            # ç­‰å¾…APIå“åº”ï¼Œå¢åŠ ç­‰å¾…æ—¶é—´åˆ°60ç§’
            wait_time = 0
            max_wait = 60
            while wait_time < max_wait and len(screener_responses) == 0:
                await asyncio.sleep(1)
                wait_time += 1
                if wait_time % 10 == 0:  # æ¯10ç§’æ˜¾ç¤ºä¸€æ¬¡ç­‰å¾…çŠ¶æ€
                    print(f"â³ ç»§ç»­ç­‰å¾…APIå“åº”... ({wait_time}/{max_wait}ç§’)")
                    print(f"ğŸ“Š å½“å‰æ•è·çŠ¶æ€: è¯·æ±‚={len(screener_requests)}, å“åº”={len(screener_responses)}")

            # å¦‚æœè¿˜æ²¡æœ‰æ•è·åˆ°APIï¼Œå°è¯•è§¦å‘æ›´å¤šç½‘ç»œæ´»åŠ¨
            if len(screener_responses) == 0:
                print("ğŸ”„ å°è¯•åˆ·æ–°é¡µé¢è§¦å‘APIè¯·æ±‚...")
                await page.reload(wait_until="domcontentloaded")
                await asyncio.sleep(5)

                # å†ç­‰å¾…30ç§’
                additional_wait = 0
                while additional_wait < 30 and len(screener_responses) == 0:
                    await asyncio.sleep(1)
                    additional_wait += 1
                    if additional_wait % 10 == 0:
                        print(f"â³ åˆ·æ–°åç­‰å¾…... ({additional_wait}/30ç§’)")
                        print(f"ğŸ“Š å½“å‰æ•è·çŠ¶æ€: è¯·æ±‚={len(screener_requests)}, å“åº”={len(screener_responses)}")

            # æœ€ç»ˆçŠ¶æ€æ£€æŸ¥
            if len(screener_responses) == 0:
                print("ğŸ“‹ æœªæ•è·åˆ°APIå“åº”ï¼Œå¯èƒ½é¡µé¢ä½¿ç”¨äº†ä¸åŒçš„æ•°æ®åŠ è½½æ–¹å¼")
            else:
                print(f"âœ… æˆåŠŸæ•è·åˆ° {len(screener_responses)} ä¸ªAPIå“åº”")

            # ç¦ç”¨æ•è·
            disable_capture()
            print("ğŸ”„ åœæ­¢ fundSearchResult API æ•è·")

            # APIæ•è·å®Œæˆåï¼Œå†æ¬¡æˆªå›¾å’Œä¿å­˜é¡µé¢æºç åˆ° mapping ç›®å½•
            print("ğŸ“¸ ä¿å­˜æœ€ç»ˆçŠ¶æ€...")
            final_screenshot = MAPPING_DIR / f"final_state_{timestamp}.png"
            await page.screenshot(path=str(final_screenshot), full_page=True)
            print(f"ğŸ“¸ æœ€ç»ˆçŠ¶æ€æˆªå›¾å·²ä¿å­˜: {final_screenshot}")

            final_page_source = await page.content()
            final_html_file = MAPPING_DIR / f"final_page_{timestamp}.html"
            with open(final_html_file, 'w', encoding='utf-8') as f:
                f.write(final_page_source)
            print(f"ğŸ“„ æœ€ç»ˆé¡µé¢æºç å·²ä¿å­˜: {final_html_file}")

            # ä¿å­˜æœ€ç»ˆé¡µé¢çš„è¡¨æ ¼æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            try:
                table_data = await page.evaluate('''
                    () => {
                        const tables = document.querySelectorAll('table');
                        const tableData = [];
                        tables.forEach((table, index) => {
                            const rows = Array.from(table.querySelectorAll('tr'));
                            const tableInfo = {
                                tableIndex: index,
                                rowCount: rows.length,
                                columnCount: rows[0] ? rows[0].querySelectorAll('td, th').length : 0,
                                headers: [],
                                data: []
                            };

                            // æå–è¡¨å¤´
                            const headerRow = table.querySelector('thead tr, tr:first-child');
                            if (headerRow) {
                                tableInfo.headers = Array.from(headerRow.querySelectorAll('th, td')).map(cell => cell.textContent.trim());
                            }

                            // æå–å‰10è¡Œæ•°æ®ä½œä¸ºæ ·æœ¬
                            const dataRows = Array.from(table.querySelectorAll('tbody tr, tr')).slice(0, 10);
                            tableInfo.data = dataRows.map(row =>
                                Array.from(row.querySelectorAll('td, th')).map(cell => cell.textContent.trim())
                            );

                            tableData.push(tableInfo);
                        });
                        return tableData;
                    }
                ''')

                if table_data and len(table_data) > 0:
                    table_file = RESULT_DIR / f"table_data_{timestamp}.json"
                    with open(table_file, 'w', encoding='utf-8') as f:
                        json.dump(table_data, f, indent=2, ensure_ascii=False)
                    print(f"ğŸ“Š è¡¨æ ¼æ•°æ®å·²ä¿å­˜: {table_file}")

            except Exception as e:
                print(f"âš ï¸  æå–è¡¨æ ¼æ•°æ®å¤±è´¥: {e}")

            # æ‹æ‘„æœ€ç»ˆçŠ¶æ€æˆªå›¾å¹¶ä¿å­˜é¡µé¢æºç 
            if save_to_dir:
                # ä¿å­˜æˆªå›¾åˆ° result ç›®å½•
                screenshot_file = RESULT_DIR / f"fund_screener_result_{timestamp}.png"
                await page.screenshot(path=str(screenshot_file), full_page=True)
                print(f"ğŸ“¸ ç­›é€‰ç»“æœæˆªå›¾å·²ä¿å­˜: {screenshot_file}")

                # ä¿å­˜é¡µé¢æºç åˆ° result ç›®å½•
                page_source = await page.content()
                html_file = RESULT_DIR / f"fund_screener_page_{timestamp}.html"
                with open(html_file, 'w', encoding='utf-8') as f:
                    f.write(page_source)
                print(f"ğŸ“„ é¡µé¢æºç å·²ä¿å­˜: {html_file}")

                # ä¿å­˜é¡µé¢ä¿¡æ¯
                page_info = {
                    "timestamp": timestamp,
                    "url": page.url,
                    "title": await page.title(),
                    "viewport": page.viewport_size,
                    "user_agent": await page.evaluate("navigator.userAgent"),
                    "page_size": len(page_source),
                    "description": "HSBC Fund Screener Result Page - 50 records with Speculative-5 filter"
                }

                page_info_file = RESULT_DIR / f"page_info_{timestamp}.json"
                with open(page_info_file, 'w', encoding='utf-8') as f:
                    json.dump(page_info, f, indent=2, ensure_ascii=False)
                print(f"ğŸ“‹ é¡µé¢ä¿¡æ¯å·²ä¿å­˜: {page_info_file}")

        except Exception as e:
            print(f"âŒ æ“ä½œåŸºé‡‘ç­›é€‰é¡µé¢æ—¶å‡ºé”™: {e}")
            # æ‹æ‘„é”™è¯¯çŠ¶æ€æˆªå›¾å’Œä¿å­˜é”™è¯¯é¡µé¢æºç 
            if save_to_dir:
                error_screenshot = LOG_DIR / f"error_screenshot_{timestamp}.png"
                try:
                    await page.screenshot(path=str(error_screenshot), full_page=True)
                    print(f"ğŸ“¸ é”™è¯¯çŠ¶æ€æˆªå›¾å·²ä¿å­˜: {error_screenshot}")

                    # ä¿å­˜é”™è¯¯é¡µé¢æºç 
                    error_page_source = await page.content()
                    error_html_file = LOG_DIR / f"error_page_{timestamp}.html"
                    with open(error_html_file, 'w', encoding='utf-8') as f:
                        f.write(error_page_source)
                    print(f"ğŸ“„ é”™è¯¯é¡µé¢æºç å·²ä¿å­˜: {error_html_file}")
                except Exception as e:
                    print(f"âš ï¸  ä¿å­˜é”™è¯¯çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")

        await context.close()
        await browser.close()

    return {
        "requests": screener_requests,
        "responses": screener_responses,
        "debug_logs": debug_logs,
        "api_captured": len(screener_responses) > 0
    }

async def main(product_code: str = DEFAULT_PRODUCT_CODE, item_number: int = 1, headless: bool = True):
    """ä¸»å‡½æ•°ï¼šè®¿é—®åŸºé‡‘è¯¦æƒ…é¡µé¢å¹¶æ•è·ç½‘ç»œè¯·æ±‚"""

    # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„ç½‘ç»œè¯·æ±‚å­˜å‚¨
    task_network_requests: List[Dict[str, Any]] = []
    task_network_responses: List[Dict[str, Any]] = []

    async def capture_request(request: Request):
        """æ•è·HTTPè¯·æ±‚ï¼ˆä»»åŠ¡çº§åˆ«ï¼‰"""
        try:
            # è¿‡æ»¤é™æ€èµ„æº
            if not should_capture_request(request):
                return

            request_data = {
                "timestamp": datetime.now().isoformat(),
                "method": request.method,
                "url": request.url,
                "headers": dict(request.headers),
                "resource_type": request.resource_type,
                "post_data": request.post_data if request.post_data else None,
                "is_navigation_request": request.is_navigation_request()
            }
            task_network_requests.append(request_data)
            print(f"ğŸ“¤ [{product_code}] è¯·æ±‚: {request.method} {request.url}")
        except Exception as e:
            print(f"âš ï¸  [{product_code}] æ•è·è¯·æ±‚æ—¶å‡ºé”™: {e}")

    async def capture_response(response: Response):
        """æ•è·HTTPå“åº”ï¼ˆä»»åŠ¡çº§åˆ«ï¼‰"""
        try:
            # è¿‡æ»¤é™æ€èµ„æº
            if not should_capture_response(response):
                return

            # è·å–å“åº”ä½“ï¼ˆä»…å¯¹ç‰¹å®šç±»å‹ï¼‰
            response_body = None
            content_type = response.headers.get("content-type", "")

            if any(t in content_type.lower() for t in ["json", "text", "xml", "html"]):
                try:
                    response_body = await response.text()
                except:
                    response_body = None

            response_data = {
                "timestamp": datetime.now().isoformat(),
                "url": response.url,
                "status": response.status,
                "status_text": response.status_text,
                "headers": dict(response.headers),
                "content_type": content_type,
                "response_body": response_body,
                "ok": response.ok,
                "from_service_worker": response.from_service_worker
            }
            task_network_responses.append(response_data)
            print(f"ğŸ“¥ [{product_code}] å“åº”: {response.status} {response.url}")
        except Exception as e:
            print(f"âš ï¸  [{product_code}] æ•è·å“åº”æ—¶å‡ºé”™: {e}")

    fund_detail_url = FUND_DETAIL_URL_TEMPLATE.format(product_code=product_code)

    print("="*60)
    print(f"ğŸš€ å¼€å§‹æ•è·åŸºé‡‘è¯¦æƒ…é¡µé¢ç½‘ç»œè¯·æ±‚")
    print(f"ğŸ“‹ äº§å“ä»£ç : {product_code}")
    print(f"ğŸŒ ç›®æ ‡URL: {fund_detail_url}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {DATA_DIR}")
    print("="*60)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=headless)
        context = await browser.new_context()
        page = await context.new_page()

        # è®¾ç½®ç½‘ç»œç›‘å¬å™¨ï¼ˆä½¿ç”¨ä»»åŠ¡çº§åˆ«çš„æ•è·å‡½æ•°ï¼‰
        page.on("request", capture_request)
        page.on("response", capture_response)

        print(f"\n=== æ­¥éª¤ 1: è®¿é—®åŸºé‡‘è¯¦æƒ…é¡µé¢ ===")
        print(f"ğŸŒ æ­£åœ¨è®¿é—®: {fund_detail_url}")

        try:
            # è®¿é—®åŸºé‡‘è¯¦æƒ…é¡µé¢
            await page.goto(fund_detail_url, wait_until="domcontentloaded", timeout=60000)

            # ç­‰å¾…ç½‘ç»œè¯·æ±‚å®Œæˆ
            print("â³ ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ...")
            try:
                await page.wait_for_load_state("networkidle", timeout=30000)
            except:
                print("âš ï¸  ç½‘ç»œç©ºé—²ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­æ‰§è¡Œ")
                await page.wait_for_timeout(5000)

            # éªŒè¯é¡µé¢åŠ è½½æˆåŠŸ
            page_title = await page.title()
            print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {page_title}")

            # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«åŸºé‡‘ç›¸å…³å†…å®¹
            if product_code in page_title or "fund" in page_title.lower() or "åŸºé‡‘" in page_title:
                print("âœ… é¡µé¢åŠ è½½æˆåŠŸï¼ŒåŒ…å«åŸºé‡‘ç›¸å…³å†…å®¹")
            else:
                print("âš ï¸  é¡µé¢æ ‡é¢˜æœªåŒ…å«é¢„æœŸçš„åŸºé‡‘ä¿¡æ¯")

            # ç­‰å¾…é¢å¤–æ—¶é—´ç¡®ä¿æ‰€æœ‰å¼‚æ­¥è¯·æ±‚å®Œæˆ
            print("â³ ç­‰å¾…å¼‚æ­¥è¯·æ±‚å®Œæˆ...")
            await page.wait_for_timeout(10000)

            # æ‹æ‘„é¡µé¢æˆªå›¾ - å…ˆåˆ›å»ºä¸´æ—¶æ–‡ä»¶ï¼Œç¨åç§»åŠ¨åˆ°ä¼šè¯ç›®å½•
            temp_screenshot_file = DATA_DIR / f"fund_detail_{product_code}_{timestamp}.png"
            await page.screenshot(path=str(temp_screenshot_file), full_page=True, scale="css")
            print(f"ğŸ“¸ é¡µé¢æˆªå›¾å·²ä¿å­˜: {temp_screenshot_file}")

        except Exception as e:
            print(f"âŒ è®¿é—®é¡µé¢æ—¶å‡ºé”™: {e}")

        print(f"\n=== æ­¥éª¤ 2: ä¿å­˜ç½‘ç»œæ•°æ® ===")
        result = await save_network_data(product_code, item_number, task_network_requests, task_network_responses)

        print(f"\nğŸ“Š ç½‘ç»œè¯·æ±‚ç»Ÿè®¡:")
        print(f"   ğŸ“¤ æ€»è¯·æ±‚æ•°: {len(task_network_requests)}")
        print(f"   ğŸ“¥ æ€»å“åº”æ•°: {len(task_network_responses)}")

        if task_network_requests:
            domains = set([req["url"].split("/")[2] for req in task_network_requests if "url" in req])
            methods = set([req["method"] for req in task_network_requests if "method" in req])
            print(f"   ğŸŒ æ¶‰åŠåŸŸå: {len(domains)} ä¸ª")
            print(f"   ğŸ”§ è¯·æ±‚æ–¹æ³•: {', '.join(methods)}")

        if task_network_responses:
            status_codes = set([resp["status"] for resp in task_network_responses if "status" in resp])
            print(f"   ğŸ“Š å“åº”çŠ¶æ€ç : {', '.join(map(str, status_codes))}")

        await context.close()
        await browser.close()

        if result and result.get("complete", True) is False:
            # æ ‡è®°æœªå®Œæˆï¼ŒæŠ›å‡ºå¼‚å¸¸è®©ä¸Šå±‚å¤„ç†ï¼ˆå¹¶å‘æ± è®°å½•ä¸ºå¤±è´¥ï¼Œä¸äº§å‡ºæœ€ç»ˆç›®å½•ï¼‰
            missing = result.get("missing")
            raise RuntimeError(f"å®Œæ•´æ€§æ ¡éªŒå¤±è´¥ï¼Œç¼ºå°‘API: {missing}")

        print(f"\nğŸ¯ ä»»åŠ¡å®Œæˆï¼æ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ°: {DATA_DIR}")
        return result

async def main_fund_screener(headless: bool = True):
    """ä¸»å‡½æ•°ï¼šæ•è·åŸºé‡‘ Risk return profile -> 10 year é¡µé¢çš„ API"""

    print("="*60)
    print(f"ğŸš€ åŸºé‡‘ Risk Return Profile -> 10 year API æ•è·æ¨¡å¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {FUND_SEARCH_OUTPUT_DIR}")
    print("="*60)

    # æ¸…ç†ä¹‹å‰çš„ç»“æœæ–‡ä»¶
    cleanup_previous_results()

    # ä½¿ç”¨å›ºå®šçš„è¾“å‡ºç›®å½•
    screener_dir = FUND_SEARCH_OUTPUT_DIR

    # æ•è· fundSearchResult API
    result = await capture_fund_screener_api(headless=headless, save_to_dir=screener_dir)

    if not result["api_captured"]:
        print("âŒ æœªèƒ½æ•è·åˆ° fundSearchResult API")
        return False

    # ä¿å­˜è¯·æ±‚å’Œå“åº”æ•°æ®
    requests = result["requests"]
    responses = result["responses"]
    debug_logs = result["debug_logs"]

    print(f"\n=== ä¿å­˜ fundSearchResult API æ•°æ® ===")
    print(f"ğŸ“¤ æ•è·åˆ° {len(requests)} ä¸ª fundSearchResult è¯·æ±‚")
    print(f"ğŸ“¥ æ•è·åˆ° {len(responses)} ä¸ª fundSearchResult å“åº”")
    print(f"ğŸ” è®°å½•äº† {len(debug_logs)} æ¡è°ƒè¯•æ—¥å¿—")

    # ä¿å­˜ fundSearchResult è¯·æ±‚æ•°æ®ï¼ˆshow 50 çš„è¯·æ±‚ï¼‰
    if requests:
        # æ‰¾åˆ°æœ€åä¸€ä¸ªè¯·æ±‚ï¼ˆåº”è¯¥æ˜¯ show 50 çš„è¯·æ±‚ï¼‰
        last_request = requests[-1]
        request_file = RESULT_DIR / "fundSearchResult_request.json"

        # è§£æè¯·æ±‚ä½“
        request_body = None
        if last_request.get("post_data"):
            try:
                request_body = json.loads(last_request["post_data"])
            except:
                request_body = last_request["post_data"]

        request_data = {
            "url": last_request["url"],
            "method": last_request["method"],
            "headers": last_request["headers"],
            "body": request_body,
            "timestamp": last_request["timestamp"],
            "description": "HSBC fundSearchResult API Request - Show 50 records per page with Speculative-5 risk level filter"
        }

        with open(request_file, 'w', encoding='utf-8') as f:
            json.dump(request_data, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ fundSearchResult è¯·æ±‚å·²ä¿å­˜: {request_file}")

    # ä¿å­˜ fundSearchResult å“åº”æ•°æ®ï¼ˆshow 50 çš„å“åº”ï¼‰
    if responses:
        # æ‰¾åˆ°æœ€åä¸€ä¸ªå“åº”ï¼ˆåº”è¯¥æ˜¯ show 50 çš„å“åº”ï¼‰
        last_response = responses[-1]
        response_file = RESULT_DIR / "fundSearchResult_response.json"

        # è§£æå“åº”ä½“
        response_body = None
        if last_response.get("response_body"):
            try:
                response_body = json.loads(last_response["response_body"])
            except:
                response_body = last_response["response_body"]

        response_data = {
            "responseInfo": {
                "status": last_response["status"],
                "statusText": last_response["status_text"],
                "timestamp": last_response["timestamp"],
                "url": last_response["url"],
                "bodyLength": len(last_response.get("response_body", "")),
                "description": "HSBC fundSearchResult API Response - 50 records per page with Speculative-5 risk level filter"
            },
            "responseHeaders": last_response["headers"],
            "responseData": response_body
        }

        with open(response_file, 'w', encoding='utf-8') as f:
            json.dump(response_data, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ fundSearchResult å“åº”å·²ä¿å­˜: {response_file}")

        # å¦‚æœå“åº”ä½“æ˜¯ JSONï¼Œé¢å¤–ä¿å­˜çº¯å‡€çš„å“åº”æ•°æ®
        if response_body and isinstance(response_body, dict):
            clean_response_file = RESULT_DIR / "fundSearchResult_data.json"
            with open(clean_response_file, 'w', encoding='utf-8') as f:
                json.dump(response_body, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ fundSearchResult çº¯å‡€æ•°æ®å·²ä¿å­˜: {clean_response_file}")

    # ä¿å­˜è°ƒè¯•æ—¥å¿—ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
    if debug_logs:
        debug_log_file = LOG_DIR / "debug_logs.txt"

        # å†™å…¥æ–‡æœ¬æ ¼å¼çš„è°ƒè¯•æ—¥å¿—
        with open(debug_log_file, 'w', encoding='utf-8') as f:
            f.write(f"HSBC Fund Screener Debug Log\n")
            f.write(f"Generated: {timestamp}\n")
            f.write(f"Total Log Entries: {len(debug_logs)}\n")
            f.write("=" * 80 + "\n\n")

            for i, log_entry in enumerate(debug_logs, 1):
                f.write(f"Entry #{i:02d}:\n")
                # ç¡®ä¿ log_entry æ˜¯å­—ç¬¦ä¸²
                if isinstance(log_entry, str):
                    f.write(log_entry)
                else:
                    f.write(str(log_entry))
                f.write("\n" + "-" * 60 + "\n\n")

        print(f"ğŸ” è°ƒè¯•æ—¥å¿—å·²ä¿å­˜: {debug_log_file}")

        # ç”Ÿæˆè°ƒè¯•æ‘˜è¦ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
        debug_summary_file = LOG_DIR / "debug_summary.txt"
        step_counts = {}
        action_counts = {}

        for log_entry in debug_logs:
            # ç¡®ä¿ log_entry æ˜¯å­—ç¬¦ä¸²
            if not isinstance(log_entry, str):
                log_entry = str(log_entry)

            # ä»æ—¥å¿—è¡Œä¸­æå–æ­¥éª¤å’ŒåŠ¨ä½œä¿¡æ¯
            if "] " in log_entry and " - " in log_entry:
                try:
                    header = log_entry.split("] ")[1].split("\n")[0]
                    if " - " in header:
                        step, action = header.split(" - ", 1)
                        step_counts[step] = step_counts.get(step, 0) + 1
                        action_counts[action] = action_counts.get(action, 0) + 1
                except:
                    pass

        with open(debug_summary_file, 'w', encoding='utf-8') as f:
            f.write(f"HSBC Fund Screener Debug Summary\n")
            f.write(f"Generated: {timestamp}\n")
            f.write("=" * 50 + "\n\n")

            f.write("æœ‰æ•ˆå…ƒç´ é€‰æ‹©å™¨æ€»ç»“:\n")
            f.write("1. é£é™©åå¥½ä¸‹æ‹‰æ¡†: #dropdown-riskLevel (Aæ ‡ç­¾)\n")
            f.write("2. é£é™©ç­‰çº§é€‰é¡¹: li:has-text(\"Speculative - 5\")\n")
            f.write("3. æœç´¢æŒ‰é’®: a[data-testid=\"Search\"] (Aæ ‡ç­¾)\n")
            f.write("4. ä½¿ç”¨é»˜è®¤æ˜¾ç¤ºæ•°é‡ (æ— éœ€é¢å¤–æ“ä½œ)\n\n")

            f.write("æ­¥éª¤æ‰§è¡Œç»Ÿè®¡:\n")
            for step, count in sorted(step_counts.items()):
                f.write(f"  {step}: {count} ä¸ªæ“ä½œ\n")

            f.write("\næ“ä½œç±»å‹ç»Ÿè®¡:\n")
            for action, count in sorted(action_counts.items()):
                f.write(f"  {action}: {count} æ¬¡\n")

            f.write(f"\næ€»è°ƒè¯•æ¡ç›®: {len(debug_logs)}\n")

        print(f"ğŸ“Š è°ƒè¯•æ‘˜è¦å·²ä¿å­˜: {debug_summary_file}")

    # ä¿å­˜è°ƒè¯•æ—¥å¿—åˆ° log ç›®å½•
    if debug_logs:
        debug_log_file = LOG_DIR / "debug_logs.txt"

        # å†™å…¥æ–‡æœ¬æ ¼å¼çš„è°ƒè¯•æ—¥å¿—
        with open(debug_log_file, 'w', encoding='utf-8') as f:
            f.write(f"HSBC Fund Screener Debug Log\n")
            f.write(f"Generated: {timestamp}\n")
            f.write(f"Total Log Entries: {len(debug_logs)}\n")
            f.write("=" * 80 + "\n\n")

            for i, log_entry in enumerate(debug_logs, 1):
                f.write(f"Entry #{i:02d}:\n")
                # ç¡®ä¿ log_entry æ˜¯å­—ç¬¦ä¸²
                if isinstance(log_entry, str):
                    f.write(log_entry)
                else:
                    f.write(str(log_entry))
                f.write("\n" + "-" * 60 + "\n\n")

        print(f"ğŸ” è°ƒè¯•æ—¥å¿—å·²ä¿å­˜: {debug_log_file}")



    # ä¿å­˜æ±‡æ€»ä¿¡æ¯
    summary_file = FUND_SEARCH_OUTPUT_DIR / "summary.json"
    files_created = []

    if requests:
        files_created.append("result/fundSearchResult_request.json")
    if responses:
        files_created.append("result/fundSearchResult_response.json")
        if responses and responses[-1].get("response_body"):
            try:
                json.loads(responses[-1]["response_body"])
                files_created.append("result/fundSearchResult_data.json")
            except:
                pass
    if debug_logs:
        files_created.append("log/debug_logs.txt")
        files_created.append("log/debug_summary.txt")

    # æ·»åŠ æ–°å¢çš„æ–‡ä»¶
    files_created.extend([
        f"result/fund_screener_result_{timestamp}.png",
        f"result/fund_screener_page_{timestamp}.html",
        f"result/page_info_{timestamp}.json",
        f"result/final_state_{timestamp}.png",
        f"result/final_page_{timestamp}.html"
    ])

    # æ£€æŸ¥æ˜¯å¦æœ‰è¡¨æ ¼æ•°æ®æ–‡ä»¶
    table_file = RESULT_DIR / f"table_data_{timestamp}.json"
    if table_file.exists():
        files_created.append(f"result/table_data_{timestamp}.json")

    summary_data = {
        "timestamp": timestamp,
        "mode": "fund_screener",
        "api_name": FUND_SEARCH_RESULT_API,
        "target_url": "https://investments3.personal-banking.hsbc.com.hk/shp/wealth-mobile-mds-shp-api-hk-hbap-prod-proxy/v0/wmds/fundSearchResult",
        "total_requests": len(requests),
        "total_responses": len(responses),
        "success": True,
        "files_created": files_created
    }

    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, indent=2, ensure_ascii=False)
    print(f"ğŸ“Š æ±‡æ€»ä¿¡æ¯å·²ä¿å­˜: {summary_file}")

    print(f"\nğŸ¯ åŸºé‡‘ Risk Return Profile -> 10 year API æ•è·å®Œæˆï¼")
    print(f"ğŸ“ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {screener_dir}")

    return True

async def process_single_product(product_code: str, item_number: int, headless: bool = True):
    """å¤„ç†å•ä¸ªäº§å“ä»£ç """
    print(f"\n{'='*60}")
    print(f"ğŸ¯ å¤„ç†äº§å“ #{item_number}: {product_code}")
    print(f"{'='*60}")

    try:
        await main(product_code=product_code, item_number=item_number, headless=headless)
        print(f"âœ… äº§å“ {product_code} å¤„ç†å®Œæˆ")
        return True
    except Exception as e:
        print(f"âŒ äº§å“ {product_code} å¤„ç†å¤±è´¥: {e}")
        return False

async def initialize_work_queue(products: List[Tuple[str, int]]):
    """åˆå§‹åŒ–å·¥ä½œé˜Ÿåˆ—ï¼šä¸ºæ‰€æœ‰äº§å“åˆ›å»º .init çŠ¶æ€ç›®å½•"""
    print(f"\nğŸ”§ åˆå§‹åŒ–å·¥ä½œé˜Ÿåˆ—: {len(products)} ä¸ªäº§å“")

    for product_code, item_number in products:
        init_dir = DATA_DIR / f".{item_number:03d}_{product_code}.init"
        init_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºçŠ¶æ€æ–‡ä»¶
        status_file = init_dir / "status.json"
        with open(status_file, 'w', encoding='utf-8') as f:
            json.dump({
                "product_code": product_code,
                "item_number": item_number,
                "status": "init",
                "created_at": datetime.now().isoformat(),
                "retry_count": 0,
                "max_retries": 3
            }, f, indent=2, ensure_ascii=False)

    print(f"âœ… å·¥ä½œé˜Ÿåˆ—åˆå§‹åŒ–å®Œæˆ")

async def claim_next_work() -> Tuple[str, int, Path]:
    """ä¸»åŠ¨è·å–ä¸‹ä¸€ä¸ªå·¥ä½œé¡¹ï¼šåŸå­åœ°å°† .init é‡å‘½åä¸º .working"""
    init_dirs = list(DATA_DIR.glob("*.init"))

    for init_dir in init_dirs:
        # è§£æç›®å½•åè·å–äº§å“ä¿¡æ¯
        dir_name = init_dir.name
        if not dir_name.startswith('.') or not dir_name.endswith('.init'):
            continue

        try:
            # è§£æ .NNN_ProductCode.init
            base_name = dir_name[1:-5]  # å»æ‰å¼€å¤´çš„ . å’Œç»“å°¾çš„ .init
            parts = base_name.split('_', 1)
            if len(parts) != 2:
                continue
            item_number = int(parts[0])
            product_code = parts[1]

            # åŸå­é‡å‘½åä¸º .working
            working_dir = DATA_DIR / f".{item_number:03d}_{product_code}.working"
            try:
                init_dir.rename(working_dir)
                print(f"ğŸ”’ è·å–å·¥ä½œ: #{item_number} {product_code}")
                return product_code, item_number, working_dir
            except FileExistsError:
                # å·²è¢«å…¶ä»–workerè·å–ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª
                continue
        except (ValueError, IndexError):
            # è§£æå¤±è´¥ï¼Œè·³è¿‡
            continue

    # æ²¡æœ‰å¯ç”¨å·¥ä½œ
    return None, None, None

async def complete_work(product_code: str, item_number: int, working_dir: Path, success: bool, error: str = None):
    """å®Œæˆå·¥ä½œï¼šå°† .working é‡å‘½åä¸º .done æˆ– .error"""
    status_file = working_dir / "status.json"

    # è¯»å–å½“å‰çŠ¶æ€
    try:
        with open(status_file, 'r', encoding='utf-8') as f:
            status = json.load(f)
    except:
        status = {"retry_count": 0, "max_retries": 3}

    if success:
        # æˆåŠŸï¼šé‡å‘½åä¸º .done
        done_dir = DATA_DIR / f"{item_number:03d}_{product_code}.done"
        status.update({
            "status": "done",
            "completed_at": datetime.now().isoformat(),
            "success": True
        })
        with open(status_file, 'w', encoding='utf-8') as f:
            json.dump(status, f, indent=2, ensure_ascii=False)

        try:
            working_dir.rename(done_dir)
            print(f"âœ… å®Œæˆå·¥ä½œ: #{item_number} {product_code}")
        except Exception as e:
            print(f"âš ï¸ é‡å‘½åä¸ºdoneå¤±è´¥: {e}")
    else:
        # å¤±è´¥ï¼šæ£€æŸ¥é‡è¯•æ¬¡æ•°
        retry_count = status.get("retry_count", 0)
        max_retries = status.get("max_retries", 3)

        if retry_count < max_retries:
            # é‡è¯•ï¼šé‡å‘½åå› .init
            retry_count += 1
            status.update({
                "status": "init",
                "retry_count": retry_count,
                "last_error": error,
                "last_attempt_at": datetime.now().isoformat()
            })
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump(status, f, indent=2, ensure_ascii=False)

            init_dir = DATA_DIR / f".{item_number:03d}_{product_code}.init"
            try:
                working_dir.rename(init_dir)
                print(f"ğŸ”„ é‡è¯•å·¥ä½œ: #{item_number} {product_code} (ç¬¬{retry_count}æ¬¡)")
            except Exception as e:
                print(f"âš ï¸ é‡å‘½åä¸ºinitå¤±è´¥: {e}")
        else:
            # è¶…è¿‡é‡è¯•æ¬¡æ•°ï¼šé‡å‘½åä¸º .error
            error_dir = DATA_DIR / f"{item_number:03d}_{product_code}.error"
            status.update({
                "status": "error",
                "completed_at": datetime.now().isoformat(),
                "success": False,
                "final_error": error,
                "retry_count": retry_count
            })
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump(status, f, indent=2, ensure_ascii=False)

            try:
                working_dir.rename(error_dir)
                print(f"âŒ é”™è¯¯å·¥ä½œ: #{item_number} {product_code} (é‡è¯•{retry_count}æ¬¡åå¤±è´¥)")
            except Exception as e:
                print(f"âš ï¸ é‡å‘½åä¸ºerrorå¤±è´¥: {e}")

async def worker_process(worker_id: int, headless: bool = True):
    """å·¥ä½œè¿›ç¨‹ï¼šä¸»åŠ¨è·å–å¹¶å¤„ç†å·¥ä½œé¡¹"""
    print(f"ğŸ”§ å¯åŠ¨å·¥ä½œè¿›ç¨‹ #{worker_id}")

    while True:
        # ä¸»åŠ¨è·å–å·¥ä½œ
        product_code, item_number, working_dir = await claim_next_work()

        if product_code is None:
            # æ²¡æœ‰å¯ç”¨å·¥ä½œï¼Œç­‰å¾…ä¸€ä¸‹å†æ£€æŸ¥
            await asyncio.sleep(1)
            continue

        print(f"ğŸ¯ å·¥ä½œè¿›ç¨‹ #{worker_id} å¤„ç†: #{item_number} {product_code}")

        # å¤„ç†å·¥ä½œ
        success = False
        error = None
        try:
            success = await process_single_product(product_code, item_number, headless)
        except Exception as e:
            error = str(e)
            success = False

        # å®Œæˆå·¥ä½œ
        await complete_work(product_code, item_number, working_dir, success, error)

async def process_with_pool(products: List[Tuple[str, int]], pool_size: int, headless: bool = True):
    """ä½¿ç”¨åŸºäºæ–‡ä»¶ç³»ç»ŸçŠ¶æ€çš„å·¥ä½œé˜Ÿåˆ—å¹¶å‘æ± """
    total = len(products)
    print(f"\nğŸš€ å·¥ä½œé˜Ÿåˆ—å¹¶å‘æ± å¯åŠ¨: æ€»è®¡ {total} ä¸ªäº§å“, æ± å¤§å°={pool_size}")

    # åˆå§‹åŒ–å·¥ä½œé˜Ÿåˆ—
    await initialize_work_queue(products)

    # å¯åŠ¨å·¥ä½œè¿›ç¨‹
    workers = []
    for i in range(pool_size):
        worker = asyncio.create_task(worker_process(i + 1, headless))
        workers.append(worker)

    # ç›‘æ§è¿›åº¦
    while True:
        # ç»Ÿè®¡å„çŠ¶æ€çš„æ•°é‡
        init_count = len(list(DATA_DIR.glob("*.init")))
        working_count = len(list(DATA_DIR.glob("*.working")))
        done_count = len(list(DATA_DIR.glob("*.done")))
        error_count = len(list(DATA_DIR.glob("*.error")))

        total_processed = done_count + error_count

        if total_processed >= total:
            # å…¨éƒ¨å®Œæˆï¼Œåœæ­¢å·¥ä½œè¿›ç¨‹
            for worker in workers:
                worker.cancel()
            break

        # æ¯10ç§’æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
        print(f"ğŸ“Š è¿›åº¦: âœ…{done_count} å®Œæˆ, âŒ{error_count} é”™è¯¯, ğŸ”„{working_count} å¤„ç†ä¸­, â³{init_count} ç­‰å¾…")
        await asyncio.sleep(10)

    # ç­‰å¾…æ‰€æœ‰å·¥ä½œè¿›ç¨‹ç»“æŸ
    await asyncio.gather(*workers, return_exceptions=True)

    # ç”Ÿæˆç»“æœ
    results = []
    for status_dir in DATA_DIR.glob("*.done"):
        status_file = status_dir / "status.json"
        if status_file.exists():
            with open(status_file, 'r', encoding='utf-8') as f:
                status = json.load(f)
                results.append({
                    "product_code": status["product_code"],
                    "item_number": status["item_number"],
                    "success": True,
                    "session_dir": str(status_dir)
                })

    for status_dir in DATA_DIR.glob("*.error"):
        status_file = status_dir / "status.json"
        if status_file.exists():
            with open(status_file, 'r', encoding='utf-8') as f:
                status = json.load(f)
                results.append({
                    "product_code": status["product_code"],
                    "item_number": status["item_number"],
                    "success": False,
                    "error": status.get("final_error", "Unknown error"),
                    "session_dir": None
                })

    results.sort(key=lambda x: x["item_number"])
    success_count = sum(1 for r in results if r["success"])
    failed_count = total - success_count

    print(f"\nğŸ¯ å·¥ä½œé˜Ÿåˆ—å¤„ç†å®Œæˆ: âœ…{success_count} æˆåŠŸ, âŒ{failed_count} å¤±è´¥")
    return results

async def process_batch_concurrent(batch_products: List[Tuple[str, int]], headless: bool = True):
    """å¹¶å‘å¤„ç†ä¸€æ‰¹äº§å“ï¼ˆä¾‹å¦‚ 10 ä¸ªï¼‰ï¼Œæ¯ä¸ªä»»åŠ¡éšæœºå»¶è¿Ÿ 1-10 ç§’å¯åŠ¨ï¼Œé¿å…åŒæ—¶å¹¶å‘å°–å³°"""
    print(f"\nğŸ”„ å¹¶å‘å¤„ç†æ‰¹æ¬¡: {len(batch_products)}ä¸ªäº§å“ (éšæœºå¯åŠ¨ 1-10s)")
    for product_code, item_number in batch_products:
        print(f"   ğŸ“‹ #{item_number}: {product_code}")

    async def delayed_start(product_code: str, item_number: int, headless: bool):
        delay = random.randint(1, 10)
        print(f"â³ ä»»åŠ¡é¢„çƒ­: #{item_number} {product_code} å°†åœ¨ {delay}s åå¯åŠ¨")
        await asyncio.sleep(delay)
        return await process_single_product(product_code, item_number, headless)

    # åˆ›å»ºå¹¶å‘ä»»åŠ¡ï¼ˆå¸¦éšæœºå»¶è¿Ÿï¼‰
    tasks = []
    for product_code, item_number in batch_products:
        task = asyncio.create_task(
            delayed_start(product_code, item_number, headless),
            name=f"product_{item_number}_{product_code}"
        )
        tasks.append((task, product_code, item_number))

    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    results = []
    for task, product_code, item_number in tasks:
        try:
            success = await task
            results.append((product_code, item_number, success))
        except Exception as e:
            print(f"âŒ äº§å“ {product_code} å¤„ç†å¼‚å¸¸: {e}")
            results.append((product_code, item_number, False))

    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for _, _, success in results if success)
    failed_count = len(results) - success_count

    print(f"ğŸ¯ æ‰¹æ¬¡å¤„ç†å®Œæˆ: âœ…{success_count}ä¸ªæˆåŠŸ, âŒ{failed_count}ä¸ªå¤±è´¥")
    return results

async def process_csv_file(csv_file_path: str, headless: bool = True, start_index: int = 1, end_index: int = None, pool_size: int = 3):
    """æ‰¹é‡å¤„ç†CSVæ–‡ä»¶ä¸­çš„äº§å“ä»£ç ï¼ˆæ”¯æŒå¹¶å‘ï¼‰"""
    product_codes = read_product_codes_csv(csv_file_path)

    if not product_codes:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„äº§å“ä»£ç ")
        return

    # è¿‡æ»¤ç´¢å¼•èŒƒå›´
    if end_index is None:
        end_index = len(product_codes)

    filtered_products = [(code, idx) for code, idx in product_codes if start_index <= idx <= end_index]

    # å…¼å®¹æ—§æ—¥å¿—å­—æ®µæ‰€éœ€çš„å˜é‡åï¼ˆä»…ç”¨äºæ‰“å°ï¼‰ï¼Œä¸å¹¶å‘æ± å¤§å°ä¿æŒä¸€è‡´
    batch_size = pool_size

    print(f"\nğŸš€ å¼€å§‹å¹¶å‘æ± å¤„ç†:")
    print(f"   ğŸ“ CSVæ–‡ä»¶: {csv_file_path}")
    print(f"   ğŸ“Š æ€»äº§å“æ•°: {len(product_codes)}")
    print(f"   ğŸ¯ å¤„ç†èŒƒå›´: {start_index} - {end_index}")
    print(f"   ğŸ“‹ å®é™…å¤„ç†: {len(filtered_products)}ä¸ªäº§å“")
    print(f"   ï¿½ å¹¶å‘æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   ï¿½ğŸ“ è¾“å‡ºç›®å½•: data_{timestamp}")

    # ä½¿ç”¨å¹¶å‘æ± å¤„ç†
    results = await process_with_pool(filtered_products, pool_size, headless)

    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    success_count = sum(1 for r in results if r["success"])
    failed_count = len(results) - success_count

    report = {
        "timestamp": timestamp,
        "csv_file": csv_file_path,
        "range": {"start_index": start_index, "end_index": end_index},
        "pool_size": pool_size,
        "totals": {"success": success_count, "failed": failed_count},
        "items": results
    }

    report_file = DATA_DIR / "run_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“„ è¿è¡ŒæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    print(f"ğŸ¯ å…¨éƒ¨å®Œæˆ: âœ…{success_count}, âŒ{failed_count}")

if __name__ == "__main__":
    import sys

    # æ£€æŸ¥æ˜¯å¦æ˜¯åŸºé‡‘ç­›é€‰å™¨æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == 'screener':
        # åŸºé‡‘ç­›é€‰å™¨æ¨¡å¼
        headless = True
        if len(sys.argv) > 2 and sys.argv[2].lower() in ['false', 'no', '0']:
            headless = False

        print(f"ğŸ¯ åŸºé‡‘ Risk Return Profile -> 10 year API æ•è·æ¨¡å¼:")
        print(f"   ğŸ–¥ï¸  æ— å¤´æ¨¡å¼: {headless}")
        print(f"   ğŸ“ è¾“å‡ºç›®å½•: data_{timestamp}")

        # è¿è¡ŒåŸºé‡‘ Risk Return Profile -> 10 year æ•è·
        asyncio.run(main_fund_screener(headless=headless))

    # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†CSVæ–‡ä»¶
    elif len(sys.argv) > 1 and sys.argv[1].endswith('.csv'):
        # CSVæ‰¹é‡å¤„ç†æ¨¡å¼
        csv_file_path = sys.argv[1]

        # è·å–å¤„ç†èŒƒå›´å‚æ•°
        start_index = int(sys.argv[2]) if len(sys.argv) > 2 else 1
        end_index = int(sys.argv[3]) if len(sys.argv) > 3 else None

        # è·å–å¹¶å‘æ± å¤§å°å‚æ•°
        pool_size = int(sys.argv[4]) if len(sys.argv) > 4 and sys.argv[4].isdigit() else 3

        # è·å–æ˜¯å¦æ— å¤´æ¨¡å¼å‚æ•°
        headless = True
        if len(sys.argv) > 5 and sys.argv[5].lower() in ['false', 'no', '0']:
            headless = False

        print(f"ğŸ¯ CSVå¹¶å‘æ± å¤„ç†æ¨¡å¼:")
        print(f"   ğŸ“ CSVæ–‡ä»¶: {csv_file_path}")
        print(f"   ğŸ“Š å¤„ç†èŒƒå›´: {start_index} - {end_index if end_index else 'æœ«å°¾'}")
        print(f"   ğŸ”„ å¹¶å‘æ± å¤§å°: {pool_size}")
        print(f"   ğŸ–¥ï¸  æ— å¤´æ¨¡å¼: {headless}")

        # è¿è¡Œå¹¶å‘æ± æ‰¹é‡å¤„ç†
        asyncio.run(process_csv_file(csv_file_path, headless, start_index, end_index, pool_size))

    else:
        # å•ä¸ªäº§å“å¤„ç†æ¨¡å¼
        product_code = sys.argv[1] if len(sys.argv) > 1 else DEFAULT_PRODUCT_CODE
        item_number = int(sys.argv[2]) if len(sys.argv) > 2 and sys.argv[2].isdigit() else 1

        # è·å–æ˜¯å¦æ— å¤´æ¨¡å¼å‚æ•°
        headless = True
        if len(sys.argv) > 3 and sys.argv[3].lower() in ['false', 'no', '0']:
            headless = False

        print(f"ğŸ¯ å•ä¸ªäº§å“å¤„ç†æ¨¡å¼:")
        print(f"   ğŸ“‹ äº§å“ä»£ç : {product_code}")
        print(f"   ğŸ”¢ é¡¹ç›®ç¼–å·: {item_number}")
        print(f"   ğŸ–¥ï¸  æ— å¤´æ¨¡å¼: {headless}")
        print(f"   ğŸ“ è¾“å‡ºç›®å½•: data_{timestamp}")

        # è¿è¡Œå•ä¸ªäº§å“å¤„ç†
        asyncio.run(main(product_code=product_code, item_number=item_number, headless=headless))

